{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_topkbmm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU4BcFUE5E0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cb58a493-cb7f-424b-bb84-39c9654923e7"
      },
      "source": [
        "#!pip install --upgrade cupy-cuda112==8.5.0\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tue Jun  8 22:14:21 2021       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 465.27       Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   50C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75DR78cKpM3m"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "import math\n",
        "from time import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "12phuKJIFY9A"
      },
      "source": [
        "#@title CustomKernel\n",
        "import cupy as cp\n",
        "import torch\n",
        "\n",
        "@cp.util.memoize(for_each_device=True)\n",
        "def cunnex(func_name, func_body):\n",
        "  return cp.cuda.compile_with_cache(func_body).get_function(func_name)\n",
        "\n",
        "class Stream:\n",
        "  def __init__(self, ptr):\n",
        "    self.ptr = ptr\n",
        "  \n",
        "class CustomKernel:\n",
        "  def __init__(self):\n",
        "    self._use_torch_in_cupy_malloc()\n",
        "    self.stream = Stream(torch.cuda.current_stream().cuda_stream)\n",
        "\n",
        "  @staticmethod\n",
        "  def _torch_alloc(size):\n",
        "    device = cp.cuda.Device().id\n",
        "    tensor = torch.empty(size, dtype=torch.uint8, device=device)\n",
        "    return cp.cuda.MemoryPointer(\n",
        "        cp.cuda.UnownedMemory(tensor.data_ptr(), size, tensor), 0)\n",
        "\n",
        "  def _use_torch_in_cupy_malloc(self):\n",
        "    cp.cuda.set_allocator(self._torch_alloc)\n",
        "\n",
        "  def _compile_kernel_str(\n",
        "      self,\n",
        "      kernel,\n",
        "      name,\n",
        "      options=(),\n",
        "      backend=\"nvrtc\",\n",
        "      max_dynamic_smem=None\n",
        "    ):\n",
        "    fn = cp.RawKernel(\n",
        "      kernel,\n",
        "      name,\n",
        "      options=options,\n",
        "      backend=backend,\n",
        "    )\n",
        "    if max_dynamic_smem:\n",
        "      fn.max_dynamic_shared_size_bytes = max_dynamic_smem\n",
        "    return fn"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k_w_6PgaxKF4",
        "cellView": "form"
      },
      "source": [
        "#@title bmm_helpers.cu\n",
        "kernel = \"\"\"\n",
        "#define likely(x)      __builtin_expect(!!(x), 1)\n",
        "#define unlikely(x)    __builtin_expect(!!(x), 0)\n",
        "#define load(x)        __ldcg(x)\n",
        "#define store(x, value) __stcs(x, value)\n",
        "\n",
        "#define _VOLATILE_  \n",
        "\n",
        "#define likely(x)      __builtin_expect(!!(x), 1)\n",
        "#define unlikely(x)    __builtin_expect(!!(x), 0)\n",
        "#define load(x)        __ldcg(x)\n",
        "#define store(x, value) __stcs(x, value)\n",
        "\n",
        "typedef long long ll_t;\n",
        "typedef unsigned long long ull_t;\n",
        "\n",
        "typedef struct __builtin_align__(32) {\n",
        "  float s0, s1, s2, s3, s4, s5, s6, s7;\n",
        "} _float8;\n",
        "\n",
        "typedef union {\n",
        "  _float8 f8;\n",
        "  float val[8];\n",
        "} float8;\n",
        "\n",
        "__device__ void madd(\n",
        "  float a,\n",
        "  float b,\n",
        "  float &c\n",
        ") {\n",
        "  c = fmaf(a, b, c);\n",
        "}\n",
        "\n",
        "__device__ void squared_l2(\n",
        "  float a,\n",
        "  float b,\n",
        "  float &c\n",
        "){\n",
        "  float dif = a - b;\n",
        "  c = fmaf(dif, dif, c);\n",
        "}\n",
        "\n",
        "__device__ void negative_squared_l2(\n",
        "  float a,\n",
        "  float b,\n",
        "  float &c\n",
        "){\n",
        "  float dif = a - b;\n",
        "  c = fmaf(-dif, dif, c);\n",
        "}\n",
        "\n",
        "__device__ void l1(\n",
        "  float a,\n",
        "  float b,\n",
        "  float &c\n",
        "){\n",
        "  c += fabsf(a - b);\n",
        "}\n",
        "\n",
        "__device__ void negative_l1(\n",
        "  float a,\n",
        "  float b,\n",
        "  float &c\n",
        "){\n",
        "  c -= fabsf(a - b);\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v4(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache1[8];\n",
        "  float aCache2[8];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    aCache1[mi] = aSM[0][8*vy + mi];\n",
        "  }\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    int is_odd = ki & 1;\n",
        "    if (is_odd == 0){\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache2[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache1[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    } else {\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache1[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache2[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_16_v3(\n",
        "  _VOLATILE_ float aSM[16][128+4],\n",
        "  _VOLATILE_ float bSM[16][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache[8];\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<16; ki++){\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      aCache[mi] = aSM[ki][8*vy + mi];\n",
        "    }\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "      #pragma unroll\n",
        "      for (int mi=0; mi<8; mi++){\n",
        "        float a = aCache[mi];\n",
        "        __DISTANCE_FN__(a, b, cCache[mi].val[ni]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v3(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache[8];\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      aCache[mi] = aSM[ki][8*vy + mi];\n",
        "    }\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "      #pragma unroll\n",
        "      for (int mi=0; mi<8; mi++){\n",
        "        float a = aCache[mi];\n",
        "        __DISTANCE_FN__(a, b, cCache[mi].val[ni]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void init_cCache(\n",
        "  float8 cCache[8]\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    #pragma unroll\n",
        "    for (int j=0; j<8; j++){\n",
        "      cCache[i].val[j] = 0.f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "\n",
        "// Unsafe\n",
        "__device__ void write_c(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    int iM = gStarty + vy*8 + i;\n",
        "    if (likely(iM < M)){\n",
        "      int iN_start = gStartx + vx*8;\n",
        "      reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      /*\n",
        "      if (likely(iN_start + 7 < N)){\n",
        "        reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      } else {\n",
        "        #pragma unroll\n",
        "        for (int j=0; j<8; j++){\n",
        "          int iN = iN_start + j;\n",
        "          if (iN < N){\n",
        "            C[(bid)*M*N + (iM)*N + (iN)] = cCache[i].val[j];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      */\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void write_c_v3(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  __shared__ volatile float cSM[16][128];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    int iM = gStarty + vy*8 + mi;\n",
        "    // Store 1 row from cCache to cSM\n",
        "    if (iM < M){\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        cSM[vy][vx*8 + ni] = cCache[mi].val[ni];\n",
        "      }\n",
        "      // Store to C\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        int iN = gStartx + 16*ni + vx;\n",
        "        if (iN < N){\n",
        "          float cVal = cSM[vy][16*ni + vx];\n",
        "          store(C+(bid)*M*N + (iM)*N + (iN), cVal);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  } \n",
        "}\n",
        "\n",
        "__device__ void load_ab_nn(\n",
        "  const float* A,\n",
        "  const float* B,\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4],\n",
        "  int bid, int gStartx, int gStarty, int gStartk,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  int iKA = gStartk + dx;\n",
        "  int iKB = gStartk + wy;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (likely(iM < M)){\n",
        "      if (likely(iKA < K)){\n",
        "        aBuffer1[i] = load(A + (bid)*M*K + (iM)*K + (iKA));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKA+8 < K)){\n",
        "        aBuffer2[i] = load(A + (bid)*M*K + (iM)*K + (iKA+8));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(iKB < K)){\n",
        "        bBuffer1[i] = load(B + (bid)*N*K + (iKB)*N + (iN));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKB+8 < K)){\n",
        "        bBuffer2[i] = load(B + (bid)*N*K + (iKB+8)*N + (iN));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void load_ab_tt(\n",
        "  const float* A,\n",
        "  const float* B,\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4],\n",
        "  int bid, int gStartx, int gStarty, int gStartk,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  int iKA = gStartk + wy;\n",
        "  int iKB = gStartk + dx;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + wx + i*32;\n",
        "    int iN = gStartx + dy + i*32;\n",
        "    if (likely(iM < M)){\n",
        "      if (likely(iKA < K)){\n",
        "        aBuffer1[i] = load(A + (bid)*M*K + (iKA)*M + (iM));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKA+8 < K)){\n",
        "        aBuffer2[i] = load(A + (bid)*M*K + (iKA+8)*M + (iM));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(iKB < K)){\n",
        "        bBuffer1[i] = load(B + (bid)*N*K + (iN)*K + (iKB));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKB+8 < K)){\n",
        "        bBuffer2[i] = load(B + (bid)*N*K + (iN)*K + (iKB+8));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void load_ab_nt(\n",
        "  const float* A,\n",
        "  const float* B,\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4],\n",
        "  int bid, int gStartx, int gStarty, int gStartk,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  int iKA = gStartk + dx;\n",
        "  int iKB = gStartk + dx;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + dy + i*32;\n",
        "    if (likely(iM < M)){\n",
        "      if (likely(iKA < K)){\n",
        "        aBuffer1[i] = load(A + (bid)*M*K + (iM)*K + (iKA));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKA+8 < K)){\n",
        "        aBuffer2[i] = load(A + (bid)*M*K + (iM)*K + (iKA+8));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(iKB < K)){\n",
        "        bBuffer1[i] = load(B + (bid)*N*K + (iN)*K + (iKB));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKB+8 < K)){\n",
        "        bBuffer2[i] = load(B + (bid)*N*K + (iN)*K + (iKB+8));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void load_ab_tn(\n",
        "  const float* A,\n",
        "  const float* B,\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4],\n",
        "  int bid, int gStartx, int gStarty, int gStartk,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  int iKA = gStartk + wy;\n",
        "  int iKB = gStartk + wy;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + wx + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (likely(iM < M)){\n",
        "      if (likely(iKA < K)){\n",
        "        aBuffer1[i] = load(A + (bid)*M*K + (iKA)*M + (iM));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKA+8 < K)){\n",
        "        aBuffer2[i] = load(A + (bid)*M*K + (iKA+8)*M + (iM));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(iKB < K)){\n",
        "        bBuffer1[i] = load(B + (bid)*N*K + (iKB)*N + (iN));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(iKB+8 < K)){\n",
        "        bBuffer2[i] = load(B + (bid)*N*K + (iKB+8)*N + (iN));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_nn(\n",
        "  _VOLATILE_ float aSM1[8][128+4],\n",
        "  _VOLATILE_ float aSM2[8][128+4],\n",
        "  _VOLATILE_ float bSM1[8][128+4],\n",
        "  _VOLATILE_ float bSM2[8][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM1[dx][dy+i*32] = aBuffer1[i];\n",
        "    bSM1[wy][wx+i*32+i] = bBuffer1[i];\n",
        "    aSM2[dx][dy+i*32] = aBuffer2[i];\n",
        "    bSM2[wy][wx+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_tt(\n",
        "  _VOLATILE_ float aSM1[8][128+4],\n",
        "  _VOLATILE_ float aSM2[8][128+4],\n",
        "  _VOLATILE_ float bSM1[8][128+4],\n",
        "  _VOLATILE_ float bSM2[8][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM1[wy][wx+i*32] = aBuffer1[i];\n",
        "    aSM2[wy][wx+i*32] = aBuffer2[i];\n",
        "    bSM1[dx][dy+i*32+i] = bBuffer1[i];\n",
        "    bSM2[dx][dy+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_nt(\n",
        "  _VOLATILE_ float aSM1[8][128+4],\n",
        "  _VOLATILE_ float aSM2[8][128+4],\n",
        "  _VOLATILE_ float bSM1[8][128+4],\n",
        "  _VOLATILE_ float bSM2[8][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM1[dx][dy+i*32] = aBuffer1[i];\n",
        "    aSM2[dx][dy+i*32] = aBuffer2[i];\n",
        "    bSM1[dx][dy+i*32+i] = bBuffer1[i];\n",
        "    bSM2[dx][dy+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_tn(\n",
        "  _VOLATILE_ float aSM1[8][128+4],\n",
        "  _VOLATILE_ float aSM2[8][128+4],\n",
        "  _VOLATILE_ float bSM1[8][128+4],\n",
        "  _VOLATILE_ float bSM2[8][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM1[wy][wx+i*32] = aBuffer1[i];\n",
        "    aSM2[wy][wx+i*32] = aBuffer2[i];\n",
        "    bSM1[wy][wx+i*32+i] = bBuffer1[i];\n",
        "    bSM2[wy][wx+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_16_nn(\n",
        "  _VOLATILE_ float aSM[16][128+4],\n",
        "  _VOLATILE_ float bSM[16][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM[dx][dy+i*32] = aBuffer1[i];\n",
        "    aSM[dx+8][dy+i*32] = aBuffer2[i];\n",
        "    bSM[wy][wx+i*32+i] = bBuffer1[i];\n",
        "    bSM[wy+8][wx+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_16_tt(\n",
        "  _VOLATILE_ float aSM[16][128+4],\n",
        "  _VOLATILE_ float bSM[16][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM[wy][wx+i*32] = aBuffer1[i];\n",
        "    aSM[wy+8][wx+i*32] = aBuffer2[i];\n",
        "    bSM[dx][dy+i*32+i] = bBuffer1[i];\n",
        "    bSM[dx+8][dy+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_16_nt(\n",
        "  _VOLATILE_ float aSM[16][128+4],\n",
        "  _VOLATILE_ float bSM[16][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM[dx][dy+i*32] = aBuffer1[i];\n",
        "    aSM[dx+8][dy+i*32] = aBuffer2[i];\n",
        "    bSM[dx][dy+i*32+i] = bBuffer1[i];\n",
        "    bSM[dx+8][dy+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void buffer2smem_16_tn(\n",
        "  _VOLATILE_ float aSM[16][128+4],\n",
        "  _VOLATILE_ float bSM[16][128+4],\n",
        "  float aBuffer1[4],\n",
        "  float aBuffer2[4],\n",
        "  float bBuffer1[4],\n",
        "  float bBuffer2[4]\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int wx = tid % 32;\n",
        "  int wy = tid / 32;\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    // Store buffered tiles into shared memory\n",
        "    aSM[wy][wx+i*32] = aBuffer1[i];\n",
        "    aSM[wy+8][wx+i*32] = aBuffer2[i];\n",
        "    bSM[wy][wx+i*32+i] = bBuffer1[i];\n",
        "    bSM[wy+8][wx+i*32+i] = bBuffer2[i];\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"bmm_helpers.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVay8JGQU_mr",
        "cellView": "form"
      },
      "source": [
        "#@title BitonicSort Kernel\n",
        "\n",
        "kernel = \"\"\"\n",
        "typedef long long ll_t;\n",
        "#define isnan(x) ( x != x )\n",
        "\n",
        "#if (__CUDA_ARCH__ < 700)\n",
        "__device__ void __nanosleep(unsigned int ns){\n",
        "  clock_t start_clock = clock();\n",
        "  clock_t clock_offset = 0;\n",
        "  while (clock_offset < ns)\n",
        "  {\n",
        "    clock_offset = clock() - start_clock;\n",
        "  }\n",
        "}\n",
        "#endif \n",
        "\n",
        "/*\n",
        "mutex lock code from:\n",
        "https://stackoverflow.com/questions/18963293/cuda-atomics-change-flag/18968893#18968893\n",
        "*/\n",
        "\n",
        "__device__ void mutex_lock_v2(\n",
        "  unsigned int *mutex\n",
        ") {\n",
        "  unsigned int ns = 8;\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0){\n",
        "    while (atomicCAS(mutex, 0, 1) == 1) {\n",
        "      __nanosleep(ns);\n",
        "      if (ns < 256) {\n",
        "        ns *= 2;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_lock(\n",
        "  unsigned int *mutex,\n",
        "  unsigned int blockMutex[1]\n",
        ") {\n",
        "  unsigned int ns = 8;\n",
        "  float old_value;\n",
        "  if (threadIdx.x == 0){\n",
        "    old_value = atomicCAS(mutex, 0, 1);\n",
        "    blockMutex[0] = old_value;\n",
        "  }\n",
        "  __syncthreads();\n",
        "  old_value = blockMutex[0];\n",
        "  while (old_value == 1) {\n",
        "    __nanosleep(ns);\n",
        "    if (ns < 256) {\n",
        "      ns *= 2;\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0){\n",
        "      old_value = atomicCAS(mutex, 0, 1);\n",
        "      blockMutex[0] = old_value;\n",
        "    }\n",
        "    __syncthreads();\n",
        "    old_value = blockMutex[0];\n",
        "    __syncthreads();\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock_v2(unsigned int *mutex) {\n",
        "  __threadfence();\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicExch(mutex, 0);\n",
        "    __threadfence();\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock(unsigned int *mutex) {\n",
        "  atomicExch(mutex, 0);\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ unsigned int bfe(\n",
        "  unsigned int source,\n",
        "  unsigned int bitIndex\n",
        ") {\n",
        "  unsigned int bit;\n",
        "  asm volatile(\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bit) : \"r\"((unsigned int) source), \"r\"(bitIndex), \"r\"(1));\n",
        "  return bit;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void warpComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction\n",
        "){\n",
        "  const float other_value = __shfl_xor_sync(0xFFFFFFFF, value, stride);\n",
        "  const float other_index = __shfl_xor_sync(0xFFFFFFFF, index, stride);\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void blockComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction,\n",
        "  const int laneID,\n",
        "  float valSM[128],\n",
        "  float idxSM[128]\n",
        "){\n",
        "  valSM[laneID] = value;\n",
        "  idxSM[laneID] = index;\n",
        "  __syncthreads();\n",
        "\n",
        "  float other_value = valSM[laneID ^ stride];\n",
        "  float other_index = idxSM[laneID ^ stride];\n",
        "  __syncthreads();\n",
        "\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort256(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  float valSM[128],\n",
        "  float idxSM[128],\n",
        "  int gStartx, int Q\n",
        "){\n",
        "  float other_value = values[threadIdx.x];\n",
        "  float other_index = indices[threadIdx.x] - gStartx;\n",
        "  \n",
        "  bool condition = value > other_value == 0;\n",
        "  if (condition){\n",
        "    float temp_value = value;\n",
        "    float temp_index = index;\n",
        "    value = other_value;\n",
        "    index = other_index;\n",
        "    other_value = temp_value;\n",
        "    other_index = temp_index;\n",
        "  }\n",
        "\n",
        "  int laneID = threadIdx.x % 128;\n",
        "  int i = 7;\n",
        "  for (int j = 6; j >= 0; j--){\n",
        "    unsigned int direction = bfe(laneID, 8) ^ bfe(laneID, j);\n",
        "    int stride = pow(2, j);\n",
        "    if (stride < 32){\n",
        "      warpComparator(value, index, stride, !direction);\n",
        "    } else {\n",
        "      blockComparator(value, index, stride, !direction, laneID, valSM, idxSM);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (threadIdx.x < Q){\n",
        "    values[threadIdx.x] = value;\n",
        "    indices[threadIdx.x] = index + gStartx;\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  float valSM[128],\n",
        "  float idxSM[128]\n",
        ") {\n",
        "  unsigned int laneID = threadIdx.x % 128;\n",
        "  for (int i=0; i < 7; i++){\n",
        "    for (int j=i; j >= 0; j--){\n",
        "      unsigned int direction = bfe(laneID, i + 1) ^ bfe(laneID, j);\n",
        "      int stride = pow(2, j);\n",
        "      // if (i==6 && j==0) break;\n",
        "      if (stride < 32){\n",
        "        warpComparator(value, index, stride, direction);\n",
        "      } else {\n",
        "        blockComparator(value, index, stride, direction, laneID, valSM, idxSM);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bitonic_sort(\n",
        "   const float* __restrict__ arr,\n",
        "   float* values,\n",
        "   ll_t* indices,\n",
        "   unsigned int* mutex,\n",
        "   int L, int Q\n",
        "){\n",
        "  int gStartx = blockIdx.x * 128;\n",
        "  int tid = threadIdx.x;\n",
        "  __shared__ float valSM[128];\n",
        "  __shared__ float idxSM[128];\n",
        "  \n",
        "  float value;\n",
        "  float index;\n",
        "  int iL = gStartx + tid;\n",
        "  if (iL < L){\n",
        "    value = arr[iL];\n",
        "    index = tid;\n",
        "  } else {\n",
        "    value = -INFINITY;\n",
        "  }\n",
        "  \n",
        "  bitonicSort(value, index, valSM, idxSM);\n",
        "\n",
        "  __shared__ unsigned int blockMutex[1];\n",
        "  mutex_lock_v2(mutex);\n",
        "\n",
        "  bitonicSort256(\n",
        "    value, index, values, indices,\n",
        "    valSM, idxSM, gStartx, Q\n",
        "  );\n",
        "  \n",
        "  mutex_unlock_v2(mutex);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"BitonicSort.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmlIFoKnSRs0",
        "cellView": "form"
      },
      "source": [
        "#@title BitonicSort\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class BitonicSort(CustomKernel): \n",
        "  def __init__(self):\n",
        "    super(BitonicSort, self).__init__()\n",
        "    \n",
        "    with open(\"BitonicSort.cu\",'r') as f: ###\n",
        "      self.kernel = f.read()\n",
        "    \n",
        "    self._fn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bitonic_sort\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "\n",
        "  def __call__(self, arr):\n",
        "    l = arr.shape[0]\n",
        "    q = 128\n",
        "    threads_per_block = (128,)\n",
        "    blocks_per_grid = ( math.ceil(l/128), )\n",
        "    values = torch.empty(128, device=\"cuda:0\", dtype=torch.float)\n",
        "    values.fill_(float(\"-inf\"))\n",
        "    indices = torch.empty(128, device=\"cuda:0\", dtype=torch.long)\n",
        "    mutex = torch.zeros(1, device=\"cuda:0\", dtype=torch.int)\n",
        "\n",
        "    self._fn(\n",
        "      grid = blocks_per_grid,\n",
        "      block = threads_per_block,\n",
        "      args = [\n",
        "        arr.data_ptr(),\n",
        "        values.data_ptr(),\n",
        "        indices.data_ptr(),\n",
        "        mutex.data_ptr(),\n",
        "        l, q\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    print(mutex)\n",
        "    return values, indices\n",
        "\n",
        "# x = torch.randn(128*1024, device=\"cuda:0\")\n",
        "# bitonic_sort = BitonicSort()\n",
        "\n",
        "# v1, i1 = torch.topk(x, k=128)\n",
        "# v2, i2 = bitonic_sort(x)\n",
        "# print(i1)\n",
        "# print(i2)\n",
        "\n",
        "# # plt.plot(x.cpu())\n",
        "# # plt.show()\n",
        "\n",
        "# # print(v2)\n",
        "# plt.plot(v1.cpu())\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(v2.cpu())\n",
        "# plt.show()\n",
        "\n",
        "# # x2 = x.sort(dim=0, descending=True)[0]\n",
        "# # plt.plot(x2.cpu())\n",
        "# # plt.show()\n",
        "\n",
        "\n",
        "# val_dif = (v1 - v2).abs()\n",
        "# idx_dif = (i1 != i2)\n",
        "\n",
        "# print(\"val error\", val_dif.sum())\n",
        "# print(\"idx error\", idx_dif.sum())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn_2LdNS6ibb",
        "cellView": "form"
      },
      "source": [
        "#@title TopkBMM Kernel\n",
        "kernel = \"\"\"\n",
        "#define isnan(x) ( x != x )\n",
        "#if (__CUDA_ARCH__ < 700)\n",
        "__device__ void __nanosleep(unsigned int ns){\n",
        "  clock_t start_clock = clock();\n",
        "  clock_t clock_offset = 0;\n",
        "  while (clock_offset < ns)\n",
        "  {\n",
        "    clock_offset = clock() - start_clock;\n",
        "  }\n",
        "}\n",
        "#endif \n",
        "\n",
        "__device__ void mutex_lock(\n",
        "  unsigned int *mutex\n",
        ") {\n",
        "  unsigned int ns = 8;\n",
        "  unsigned int counter = 0;\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0 ){\n",
        "    while (atomicCAS(mutex, 0, 1) == 1) {\n",
        "      __nanosleep(ns);\n",
        "      counter ++;\n",
        "      if (counter > 100000) break;\n",
        "      if (ns < 256) {\n",
        "        ns *= 2;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_lock_noop(\n",
        ") {\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock(\n",
        "  unsigned int *mutex\n",
        ") {\n",
        "  __threadfence();\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicExch(mutex, 0);\n",
        "    __threadfence();\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock_noop(){\n",
        "  __syncthreads();\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ unsigned int bfe(\n",
        "  unsigned int source,\n",
        "  unsigned int bitIndex\n",
        ") {\n",
        "  unsigned int bit;\n",
        "  asm volatile(\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bit) : \"r\"((unsigned int) source), \"r\"(bitIndex), \"r\"(1));\n",
        "  return bit;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void warpComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction\n",
        "){\n",
        "  const float other_value = __shfl_xor_sync(0xFFFFFFFF, value, stride);\n",
        "  const float other_index = __shfl_xor_sync(0xFFFFFFFF, index, stride);\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void blockComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction,\n",
        "  const int laneID,\n",
        "  _VOLATILE_ float valSM[128+4],\n",
        "  _VOLATILE_ float idxSM[128+4]\n",
        "){\n",
        "  valSM[laneID] = value;\n",
        "  idxSM[laneID] = index;\n",
        "  __syncthreads();\n",
        "\n",
        "  float other_value = valSM[laneID ^ stride];\n",
        "  float other_index = idxSM[laneID ^ stride];\n",
        "  __syncthreads();\n",
        "\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort128(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  _VOLATILE_ float valSM[128+4],\n",
        "  _VOLATILE_ float idxSM[128+4]\n",
        ") {\n",
        "  unsigned int laneID = threadIdx.x % 128;\n",
        "  warpComparator(value, index, 1, bfe(laneID, 1) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 2, bfe(laneID, 2) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 2) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 4, bfe(laneID, 3) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 3) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 3) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 8, bfe(laneID, 4) ^ bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 4) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 4) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 4) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 16, bfe(laneID, 5) ^ bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, bfe(laneID, 5) ^ bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 5) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 5) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 5) ^ bfe(laneID, 0));\n",
        "\n",
        "  blockComparator(value, index, 32, bfe(laneID, 6) ^ bfe(laneID, 5), laneID, valSM, idxSM);\n",
        "  warpComparator(value, index, 16, bfe(laneID, 6) ^ bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, bfe(laneID, 6) ^ bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 6) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 6) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 6) ^ bfe(laneID, 0));\n",
        "\n",
        "  blockComparator(value, index, 64, bfe(laneID, 6), laneID, valSM, idxSM);\n",
        "  blockComparator(value, index, 32, bfe(laneID, 5), laneID, valSM, idxSM);\n",
        "  warpComparator(value, index, 16, bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 0));\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort256(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  float* gValue,\n",
        "  ll_t* gIndex,\n",
        "  float valSM[128+4],\n",
        "  float idxSM[128+4],\n",
        "  int Q, int adr, bool ok\n",
        "){\n",
        "  int laneID = threadIdx.x % 128;\n",
        "  float other_index;\n",
        "  float other_value; \n",
        "  if (ok){\n",
        "    other_value = gValue[adr];\n",
        "    other_index = gIndex[adr];\n",
        "  } else {\n",
        "    other_value = -99999;\n",
        "    other_index = 0;\n",
        "  }\n",
        "  bool condition = value > other_value == 0;\n",
        "  if (condition){\n",
        "    value = value + other_value;\n",
        "    index = index + other_index;\n",
        "    other_value = value - other_value;\n",
        "    other_index = index - other_index;\n",
        "    value = value - other_value;\n",
        "    index = index - other_index;\n",
        "  }\n",
        "\n",
        "  blockComparator(value, index, 64, !bfe(laneID, 6), laneID, valSM, idxSM);\n",
        "  blockComparator(value, index, 32, !bfe(laneID, 5), laneID, valSM, idxSM);\n",
        "  warpComparator(value, index, 16, !bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, !bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, !bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, !bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, !bfe(laneID, 0));\n",
        "  /*\n",
        "  */\n",
        "  if (ok){\n",
        "    gValue[adr] = value;\n",
        "    gIndex[adr] = index;\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort256_noop()\n",
        "{\n",
        "  __syncthreads();\n",
        "  __syncthreads();\n",
        "  __syncthreads();\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void topk_dim_1(\n",
        "  float8 cCache[8],\n",
        "  _VOLATILE_ float valSM[16][128+4],\n",
        "  _VOLATILE_ float idxSM[16][128+4],\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int gStartx, int gStarty, int bid,\n",
        "  int M, int N, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int hx = tid % 128;\n",
        "  int hy = tid / 128;\n",
        "  #pragma unroll\n",
        "  for (int ni=0; ni<8; ni++){\n",
        "    int iN = gStartx + vx*8 + ni;\n",
        "    //if (iN < N) break;\n",
        "\n",
        "    // Store cCache to cSM\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      int iM = gStarty + vy*8 + mi;\n",
        "      if (likely(iM < M && iN < N)){\n",
        "        valSM[vx][vy*8 + mi] = cCache[mi].val[ni];\n",
        "        idxSM[vx][vy*8 + mi] = iM;\n",
        "      } else {\n",
        "        valSM[vx][vy*8 + mi] = -123456;\n",
        "        idxSM[vx][vy*8 + mi] = -1;\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Load from cSM to cCache\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<8; i++){\n",
        "      float value = valSM[hy*8 + i][hx];\n",
        "      float index = idxSM[hy*8 + i][hx];\n",
        "      bitonicSort128(\n",
        "        value, index,\n",
        "        valSM[hy*8 + i], idxSM[hy*8 + i]\n",
        "      );\n",
        "      int iN = gStartx + (hy*8 + i)*8 + ni;\n",
        "      int adr = (bid)*N*Q + iN*Q + hx;\n",
        "      mutex_lock( &mutex[(bid)*N + iN] );\n",
        "      bitonicSort256(\n",
        "        value, index, \n",
        "        values, indices, \n",
        "        valSM[hy*8+i], idxSM[hy*8+i],\n",
        "        Q, adr, iN < N\n",
        "      );\n",
        "      mutex_unlock( &mutex[(bid)*N + iN] );\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void topk_dim_2(\n",
        "  float8 cCache[8],\n",
        "  _VOLATILE_ float valSM[16][128+4],\n",
        "  _VOLATILE_ float idxSM[16][128+4],\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int gStartx, int gStarty, int bid,\n",
        "  int M, int N, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int hx = tid % 128;\n",
        "  int hy = tid / 128;\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    int iM = gStarty + vy*8 + mi;\n",
        "    //if (iM >= M) break;\n",
        "\n",
        "    // Store cCache to cSM\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      int iN = gStartx + vx*8 + ni;\n",
        "      if (likely(iN < N && iM < M)){\n",
        "        valSM[vy][vx*8 + ni] = cCache[mi].val[ni];\n",
        "        idxSM[vy][vx*8 + ni] = iN;\n",
        "      } else {\n",
        "        valSM[vy][vx*8 + ni] = -123456;\n",
        "        idxSM[vy][vx*8 + ni] = -1;\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Load from cSM to cCache\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<8; i++){\n",
        "      float value = valSM[hy*8 + i][hx];\n",
        "      float index = idxSM[hy*8 + i][hx];\n",
        "      bitonicSort128(\n",
        "        value, index,\n",
        "        valSM[hy*8 + i], idxSM[hy*8 + i]\n",
        "      );\n",
        "      int iM = gStarty + (hy*8 + i)*8 + mi;\n",
        "      int adr = (bid)*M*Q + iM*Q + hx;\n",
        "      mutex_lock( &mutex[(bid)*M + iM] );\n",
        "      bitonicSort256(\n",
        "        value, index, \n",
        "        values, indices, \n",
        "        valSM[hy*8+i], idxSM[hy*8+i],\n",
        "        Q, adr, iM < M\n",
        "      );\n",
        "      mutex_unlock( &mutex[(bid)*M + iM] );\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_tn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int M, int N, int K, int DIM, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM[16][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM[16][128+4];\n",
        "\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_tn(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "    buffer2smem_16_tn(\n",
        "      aSM, bSM,\n",
        "      aBuffer1, aBuffer2,\n",
        "      bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_tn(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    thread_matmul_16_v3(aSM, bSM, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // TopK sort along DIM\n",
        "  if (DIM == 1){\n",
        "    topk_dim_1(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  } else if (DIM == 2){\n",
        "    topk_dim_2(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_nt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int M, int N, int K, int DIM, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM[16][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM[16][128+4];\n",
        "\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_nt(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "    buffer2smem_16_nt(\n",
        "      aSM, bSM,\n",
        "      aBuffer1, aBuffer2,\n",
        "      bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_nt(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    thread_matmul_16_v3(aSM, bSM, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // TopK sort along DIM\n",
        "  if (DIM == 1){\n",
        "    topk_dim_1(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  } else if (DIM == 2){\n",
        "    topk_dim_2(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_nn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int M, int N, int K, int DIM, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM[16][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM[16][128+4];\n",
        "\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_nn(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "    buffer2smem_16_nn(\n",
        "      aSM, bSM,\n",
        "      aBuffer1, aBuffer2,\n",
        "      bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_nn(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    thread_matmul_16_v3(aSM, bSM, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // TopK sort along DIM\n",
        "  if (DIM == 1){\n",
        "    topk_dim_1(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  } else if (DIM == 2){\n",
        "    topk_dim_2(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_tt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int M, int N, int K, int DIM, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM[16][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM[16][128+4];\n",
        "\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_tt(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "    buffer2smem_16_tt(\n",
        "      aSM, bSM,\n",
        "      aBuffer1, aBuffer2,\n",
        "      bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_tt(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    thread_matmul_16_v3(aSM, bSM, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // TopK sort along DIM\n",
        "  if (DIM == 1){\n",
        "    topk_dim_1(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  } else if (DIM == 2){\n",
        "    topk_dim_2(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  }\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"TopkBMMCUDA.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm88_WSG7ig1",
        "cellView": "form"
      },
      "source": [
        "#@title TopkBMM\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class TopkBMMCUDA(CustomKernel): \n",
        "  def __init__(\n",
        "      self,\n",
        "      m=None, n=None, k=None,\n",
        "      patch_m=4, patch_n=4,\n",
        "      distance=\"inner\"\n",
        "    ):\n",
        "    super(TopkBMMCUDA, self).__init__()\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = k\n",
        "    self.patch_m = patch_m\n",
        "    self.patch_n = patch_n\n",
        "    if distance == \"inner\":\n",
        "      dist_fn = \"madd\"\n",
        "    elif distance in [\"l2\", \"euclidean\"]:\n",
        "      dist_fn = \"squared_l2\"\n",
        "    elif distance in [\"l1\", \"manhattan\"]:\n",
        "      dist_fn = \"l1\"\n",
        "    else:\n",
        "      ValueError(\"Unrecognized distance type\")\n",
        "\n",
        "    self.distance = distance\n",
        "\n",
        "\n",
        "    with open(\"bmm_helpers.cu\",'r') as f: ###\n",
        "      helpers = f.read()\n",
        "    \n",
        "    with open(\"TopkBMMCUDA.cu\",'r') as f: ###\n",
        "      self.kernel = helpers + f.read()\n",
        "      \n",
        "    self.kernel = (self.kernel\n",
        "      .replace(\"_M_\", str(m) if m else \"M\")\n",
        "      .replace(\"_N_\", str(n) if n else \"N\")\n",
        "      .replace(\"_K_\", str(k) if k else \"K\")\n",
        "      .replace(\"_PM_\", str(self.patch_m))\n",
        "      .replace(\"_PN_\", str(self.patch_n))\n",
        "      .replace(\"__DISTANCE_FN__\", dist_fn)\n",
        "    )\n",
        "    \n",
        "    self._fn_tt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_tt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_nn\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "    # print(self._fn_nn.attributes)\n",
        "    self._fn_tn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_tn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_nt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "\n",
        "  def get_mode(self, A, B):\n",
        "    mode = [None, None]\n",
        "    if A.stride()[-1] == 1:\n",
        "      mode[0] = \"n\"\n",
        "    elif A.stride()[-2] == 1:\n",
        "      mode[0] = \"t\"\n",
        "    if B.stride()[-1] == 1:\n",
        "      mode[1] = \"n\"\n",
        "    elif B.stride()[-2] == 1:\n",
        "      mode[1] = \"t\"\n",
        "    return \"\".join(mode)\n",
        "\n",
        "  def __call__(self, A, B, k=128, dim=1):\n",
        "    \"\"\"\n",
        "      Performs C = min(f(A) @ g(B)), argmin(f(A) @ g(B))\n",
        "      A: torch.Tensor, shape : [l, m, k] or [l, k, m]\n",
        "      B: torch.Tensor, shape : [l, n, k] or [l, k, n]\n",
        "      returns C: torch.Tensor, shape : [l, m, n]\n",
        "      Notes:\n",
        "        f() and g() are determined by mode\n",
        "        \"nn\" --> A @ B\n",
        "        \"tt\" --> A.T @ B.T\n",
        "        \"nt\" --> A @ B.T\n",
        "        \"tn\" --> A.T @ B\n",
        "    \"\"\"\n",
        "    assert len(A.shape) == len(B.shape)\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      A = A[None]\n",
        "      B = B[None]\n",
        "      two_dimentional = True\n",
        "      dim += 1\n",
        "    elif len(A.shape) == 3 and len(B.shape) == 3:\n",
        "      two_dimentional = False\n",
        "    else:\n",
        "      raise ValueError(\"shape of A and B need to be 2d or 3d\")\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[1]\n",
        "    assert A.dtype == B.dtype\n",
        "    assert A.dtype in [torch.float, torch.half]\n",
        "    assert A.device.type == B.device.type == \"cuda\"\n",
        "    assert dim in [1, 2]\n",
        "    assert 0 < k <= 128\n",
        "\n",
        "    mode = self.get_mode(A, B)\n",
        "    if mode == \"nn\":\n",
        "      kernel_fn = self._fn_nn\n",
        "    elif mode == \"nt\":\n",
        "      kernel_fn = self._fn_nt\n",
        "    elif mode == \"tn\":\n",
        "      kernel_fn = self._fn_tn\n",
        "    elif mode == \"tt\":\n",
        "      kernel_fn = self._fn_tt\n",
        "\n",
        "    l, m, d = A.shape\n",
        "    l, d, n = B.shape\n",
        "\n",
        "    if dim == 1:\n",
        "      values = torch.empty([l, n, 128], device=\"cuda:0\", dtype=A.dtype)\n",
        "      indices = torch.empty([l, n, 128], device=\"cuda:0\", dtype=torch.int64)\n",
        "      mutex = torch.zeros([l, n], device=\"cuda:0\", dtype=torch.int32)\n",
        "    elif dim == 2:\n",
        "      values = torch.empty([l, m, 128], device=\"cuda:0\", dtype=A.dtype)\n",
        "      indices = torch.empty([l, m, 128], device=\"cuda:0\", dtype=torch.int64)\n",
        "      mutex = torch.zeros([l, m], device=\"cuda:0\", dtype=torch.int32)\n",
        "    # values.fill_(float(\"-inf\"))\n",
        "    values.fill_(-111111)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    kernel_fn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        values.data_ptr(),\n",
        "        indices.data_ptr(),\n",
        "        mutex.data_ptr(),\n",
        "        m, n, d, dim, 128\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    indices = indices[:, :, :k]\n",
        "    values = values[:, :, :k]\n",
        "\n",
        "    if two_dimentional:\n",
        "      indices = indices[0]\n",
        "      values = values[0]\n",
        "\n",
        "    return values, indices"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDuqX_JrO-50",
        "cellView": "form"
      },
      "source": [
        "#@title BMMv2.5 Kernel\n",
        "kernel = \"\"\"\n",
        "extern \"C\"\n",
        "__global__ void bmm_tn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float aSM2[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM2[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_tn(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    #pragma unroll\n",
        "    buffer2smem_tn(\n",
        "      aSM1, aSM2, bSM1, bSM2,\n",
        "      aBuffer1, aBuffer2, bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_tn(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    // Each thread computes 8 x 8 matrix multiplication\n",
        "    // Accumulate intermediate results in cCache\n",
        "    // aSM1, bSM1, aSM2, bSM2 are consumed\n",
        "    thread_matmul_v3(aSM1, bSM1, cCache, vx, vy);\n",
        "    thread_matmul_v3(aSM2, bSM2, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "  \n",
        "  // At the end of main loop, store cCache to C\n",
        "  //write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "  write_c_v3(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float aSM2[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM2[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_nt(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    buffer2smem_nt(\n",
        "      aSM1, aSM2, bSM1, bSM2,\n",
        "      aBuffer1, aBuffer2, bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_nt(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    // Each thread computes 8 x 8 matrix multiplication\n",
        "    // Accumulate intermediate results in cCache\n",
        "    // aSM1, bSM1, aSM2, bSM2 are consumed\n",
        "    thread_matmul_v3(aSM1, bSM1, cCache, vx, vy);\n",
        "    thread_matmul_v3(aSM2, bSM2, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "  \n",
        "  // At the end of main loop, store cCache to C\n",
        "  //write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "  write_c_v3(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float aSM2[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM2[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_nn(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    #pragma unroll\n",
        "    buffer2smem_nn(\n",
        "      aSM1, aSM2, bSM1, bSM2,\n",
        "      aBuffer1, aBuffer2, bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_nn(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    // Each thread computes 8 x 8 matrix multiplication\n",
        "    // Accumulate intermediate results in cCache\n",
        "    // aSM1, bSM1, aSM2, bSM2 are consumed\n",
        "    thread_matmul_v3(aSM1, bSM1, cCache, vx, vy);\n",
        "    thread_matmul_v3(aSM2, bSM2, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "  \n",
        "  // At the end of main loop, store cCache to C\n",
        "  //write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "  write_c_v3(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float aSM2[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM2[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  load_ab_tt(\n",
        "    A, B, \n",
        "    aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "    bid, gStartx, gStarty, 0,\n",
        "    M, N, K\n",
        "  );\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    #pragma unroll\n",
        "    buffer2smem_tt(\n",
        "      aSM1, aSM2, bSM1, bSM2,\n",
        "      aBuffer1, aBuffer2, bBuffer1, bBuffer2\n",
        "    );\n",
        "    if (likely(itr < nIt - 1)){\n",
        "      load_ab_tt(\n",
        "        A, B, \n",
        "        aBuffer1, aBuffer2, bBuffer1, bBuffer2,\n",
        "        bid, gStartx, gStarty, gStartk + 16,\n",
        "        M, N, K\n",
        "      );\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    // Each thread computes 8 x 8 matrix multiplication\n",
        "    // Accumulate intermediate results in cCache\n",
        "    // aSM1, bSM1, aSM2, bSM2 are consumed\n",
        "    thread_matmul_v3(aSM1, bSM1, cCache, vx, vy);\n",
        "    thread_matmul_v3(aSM2, bSM2, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "  \n",
        "  // At the end of main loop, store cCache to C\n",
        "  //write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "  write_c_v3(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"BMMCUDAv2_5.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZvX6kh9Pt4_",
        "cellView": "form"
      },
      "source": [
        "#@title BMMv2.5\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class BMMCUDAv2_5(CustomKernel): \n",
        "  def __init__(self, m=None, n=None, k=None, patch_m=4, patch_n=4):\n",
        "    super(BMMCUDAv2_5, self).__init__()\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = k\n",
        "    self.patch_m = patch_m\n",
        "    self.patch_n = patch_n\n",
        "    \n",
        "    with open(\"bmm_helpers.cu\", \"r\") as f:\n",
        "      helpers = f.read()\n",
        "\n",
        "    with open(\"BMMCUDAv2_5.cu\",'r') as f: ###\n",
        "      self.kernel = helpers + f.read()\n",
        "      \n",
        "    self.kernel = (self.kernel\n",
        "      .replace(\"_M_\", str(m) if m else \"M\")\n",
        "      .replace(\"_N_\", str(n) if n else \"N\")\n",
        "      .replace(\"_K_\", str(k) if k else \"K\")\n",
        "      .replace(\"_PM_\", str(self.patch_m))\n",
        "      .replace(\"_PN_\", str(self.patch_n))\n",
        "      .replace(\"__DISTANCE_FN__\", \"madd\")\n",
        "    )\n",
        "    \n",
        "    self._fn_tt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nn\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "    # print(self._fn_nn.attributes)\n",
        "    self._fn_tn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "\n",
        "  def _call_nn(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A @ B\n",
        "      A: shape = [l, m, k]\n",
        "      B: shape = [l, k, n]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_nn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k,\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_tt(self, A, B):\n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_tt(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k,\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_tn(self, A, B):\n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_tn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k,\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_nt(self, A, B):\n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_nt(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k,\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def __call__(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = f(A) @ f(B)\n",
        "      A: torch.Tensor, shape : [l, m, k] or [l, k, m]\n",
        "      B: torch.Tensor, shape : [l, n, k] or [l, k, n]\n",
        "      returns C: torch.Tensor, shape : [l, m, n]\n",
        "      mode: str, default: \"nn\"\n",
        "      Notes:\n",
        "        f() and g() are determined by mode\n",
        "        \"nn\" --> A @ B\n",
        "        \"tt\" --> A.T @ B.T\n",
        "        \"nt\" --> A @ B.T\n",
        "        \"tn\" --> A.T @ B\n",
        "    \"\"\"\n",
        "    assert len(A.shape) == len(B.shape)\n",
        "    # A = A.contiguous()\n",
        "    # B = B.contiguous()\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      A = A[None]\n",
        "      B = B[None]\n",
        "    elif len(A.shape) == 3 and len(B.shape) == 3:\n",
        "      pass\n",
        "    else:\n",
        "      raise ValueError(\"A and B need to be 2d or 3d\")\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[1]\n",
        "    assert A.dtype == B.dtype\n",
        "    assert A.dtype in [torch.float, torch.half]\n",
        "    assert A.device.type == B.device.type == \"cuda\"\n",
        "\n",
        "    mode = [None, None]\n",
        "    if A.stride()[-1] == 1:\n",
        "      mode[0] = \"n\"\n",
        "    elif A.stride()[-2] == 1:\n",
        "      mode[0] = \"t\"\n",
        "    if B.stride()[-1] == 1:\n",
        "      mode[1] = \"n\"\n",
        "    elif B.stride()[-2] == 1:\n",
        "      mode[1] = \"t\"\n",
        "\n",
        "    if mode == [\"n\", \"n\"]:\n",
        "      C = self._call_nn(A, B)\n",
        "    elif mode == [\"t\",\"t\"]:\n",
        "      C = self._call_tt(A, B)\n",
        "    elif mode == [\"t\", \"n\"]:\n",
        "      C = self._call_tn(A, B)\n",
        "    elif mode == [\"n\", \"t\"]:\n",
        "      C = self._call_nt(A, B)\n",
        "\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      C = C[0]\n",
        "    return C"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asEodQyvJ_ex"
      },
      "source": [
        "## Note\n",
        "#### TopkBMM only works correctly when *M* and *N* are divisible by 128.  \n",
        "#### \"n_candidates\" is the k in topk.  \n",
        "#### *n_candidates* should be smaller than 128.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLWnHuSX9XrV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1880f9ec-0666-4f80-bd2a-a5880f6313f4"
      },
      "source": [
        "#@title# test TopkBMM\n",
        "def test_topk_bmm(l, m, n, k, mode=\"nn\", n_iter=1, dim=1, n_candidates=128, verbose=0):\n",
        "  print(f\"l={l}  m={m}  n={n}  k={k}\")\n",
        "  if mode[0] == \"n\":\n",
        "    A = torch.randn(l, m, k, device=\"cuda:0\")\n",
        "  elif mode[0] == \"t\":\n",
        "    A = torch.randn(l, k, m, device=\"cuda:0\")\n",
        "  \n",
        "  if mode[1] == \"n\":\n",
        "    B = torch.randn(l, k, n, device=\"cuda:0\")\n",
        "  elif mode[1] == \"t\":\n",
        "    B = torch.randn(l, n, k, device=\"cuda:0\")\n",
        "\n",
        "  # if dim == 1:\n",
        "  #   custom_topk_bmm = TopkBMMCUDA(patch_m=16, patch_n=1)\n",
        "  # elif dim == 2:\n",
        "  #   custom_topk_bmm = TopkBMMCUDA(patch_m=1, patch_n=16)\n",
        "  custom_topk_bmm = TopkBMMCUDA(patch_m=4, patch_n=4)\n",
        "  flop = l * m * n * k * 2 + l * m * n\n",
        "\n",
        "  if mode[0] == \"t\":\n",
        "    At = A.transpose(1, 2)\n",
        "  else: \n",
        "    At = A\n",
        "  if mode[1] == \"t\":\n",
        "    Bt = B.transpose(1, 2)\n",
        "  else:\n",
        "    Bt = B\n",
        "  #warmup\n",
        "\n",
        "  for i in range(n_iter):\n",
        "    C = torch.bmm(At, Bt)\n",
        "    C.topk(k=n_candidates, dim = dim)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "  tm = time()\n",
        "  for i in range(n_iter):\n",
        "    if dim == 1:\n",
        "      C = torch.bmm(Bt.transpose(1,2), At.transpose(1,2))\n",
        "    elif dim == 2:\n",
        "      C = torch.bmm(At, Bt)\n",
        "    C_v, C_i = C.topk(k = n_candidates, dim = -1)\n",
        "    torch.cuda.synchronize()\n",
        "  time_cost_0 = (time() - tm) / n_iter\n",
        "  flops0 = (flop / time_cost_0) / 1000**4\n",
        "  mem_0 = C.numel() * 4 + C_v.numel() * 4 + C_i.numel() * 8\n",
        "  if dim == 1:\n",
        "    pass\n",
        "    # C_v = C_v.transpose(1,2)\n",
        "    # C_i = C_i.transpose(1,2)\n",
        "  if verbose > 0:\n",
        "    print(\"time spent for torch.bmm + min:\", time_cost_0)\n",
        "    print(\"tflops:\", flops0)\n",
        "  else:\n",
        "    del C_v, C_i \n",
        "  del C\n",
        "\n",
        "  # warmup\n",
        "  for i in range(n_iter):\n",
        "    custom_topk_bmm(At, Bt, dim=dim)\n",
        "    torch.cuda.synchronize()\n",
        "  tm = time()\n",
        "  for i in range(n_iter):\n",
        "    C1_v, C1_i = custom_topk_bmm(At, Bt, dim=dim, k=n_candidates)\n",
        "    torch.cuda.synchronize()\n",
        "  time_cost_1 = (time() - tm) / n_iter\n",
        "  flops1 = (flop / time_cost_1) / 1000**4\n",
        "  mem_1 = C1_v.numel() * 4 + C1_i.numel() * 8\n",
        "\n",
        "  if verbose > 0:\n",
        "    print(\"time spent for custom_topk_bmm:\", time_cost_1)\n",
        "    print(\"tflops:\", flops1)\n",
        "  else:\n",
        "    del C1_v, C1_i\n",
        "\n",
        "  # mask = (C1_i != C_i).cpu()\n",
        "  # n_index = torch.nonzero(mask)[:, 1]\n",
        "  # print(torch.nonzero(mask))\n",
        "  # print(C_i[mask])\n",
        "  # print(C1_i[mask])\n",
        "  # v1 = C.cpu()[:, C_i.cpu()[mask], n_index]\n",
        "  # v2 = C.cpu()[:, C1_i.cpu()[mask], n_index]\n",
        "  # # print(v1)\n",
        "  # # print(v2)\n",
        "  # print(v1 - v2)\n",
        "  # print( (v1 - v2).abs().sum() )\n",
        "\n",
        "  # plt.plot(C1_v[0, 0, :].cpu(),\"r\")\n",
        "  # plt.plot(C_v[0, 0, :].cpu(),\"b\")\n",
        "  # plt.show()\n",
        "\n",
        "  if verbose > 0:\n",
        "    val_dif = (C1_v - C_v).abs()\n",
        "    idx_dif = (C1_i != C_i)\n",
        "\n",
        "    print(\"Max Val Error\", val_dif.max())\n",
        "    print(\"Val Error:\", val_dif.sum())\n",
        "    print(\"Idx Error:\", idx_dif.sum())\n",
        "    print(\"ratio:\", time_cost_1 / time_cost_0)\n",
        "\n",
        "    # union = C1_i[:, :, :, None] == C_i[:, :, None, :]\n",
        "    # union = union.sum(dim=-1)\n",
        "    # print(union.float().mean())\n",
        "\n",
        "    recall = C1_i == C_i\n",
        "    recall = recall.float().mean()\n",
        "    print(f\"recall at {n_candidates}\", recall)\n",
        "\n",
        "  # return time_cost_0, time_cost_1\n",
        "  return mem_0, mem_1\n",
        "  \n",
        "_ = test_topk_bmm(\n",
        "    1, 1024*32, 1024, 256,\n",
        "    mode=\"nn\", dim=1, n_iter=100, \n",
        "    n_candidates=128, verbose=1\n",
        ")\n",
        "# topk_dim1_v1 0.41 0.03\n",
        "# topk_dim1_v2 0.38 0.0305"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l=1  m=32768  n=1024  k=256\n",
            "time spent for torch.bmm + min: 0.01628307342529297\n",
            "tflops: 1.0571360311660751\n",
            "time spent for custom_topk_bmm: 0.010601117610931396\n",
            "tflops: 1.6237366896345242\n",
            "Max Val Error tensor(0., device='cuda:0')\n",
            "Val Error: tensor(0., device='cuda:0')\n",
            "Idx Error: tensor(4, device='cuda:0')\n",
            "ratio: 0.6510513914691543\n",
            "recall at 128 tensor(1.0000, device='cuda:0')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9-LksFDGiKF"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkMqJGQyPtj"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"imgs\"):\n",
        "  os.mkdir(\"imgs\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w4KEWVzPFwJO",
        "cellView": "form",
        "outputId": "13f97cf8-8d20-47f9-999e-649b4ec93bd0"
      },
      "source": [
        "#@title Grid test TopkBMM (Memory Footprint)\n",
        "ls = [1]\n",
        "ms = [256* 2**i for i in range(1, 13)]\n",
        "ns = [1024]\n",
        "\n",
        "ks = [1024]\n",
        "mode=\"nn\"\n",
        "\n",
        "custom_res = dict()\n",
        "cublass_res = dict()\n",
        "for l in ls:\n",
        "  for n in ns:\n",
        "    for m in ms:\n",
        "      for k in ks:     \n",
        "        res = test_topk_bmm(\n",
        "          l, m, n, k,\n",
        "          mode=mode, n_iter=1, dim=1,\n",
        "          n_candidates = 128,\n",
        "        )\n",
        "        cublass_res[m] = res[0] / 1024**2\n",
        "        custom_res[m] = res[1] / 1024**2\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10) )\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"N\", fontsize=17)\n",
        "plt.ylabel(\"MB\", fontsize=17)\n",
        "title = f\"A[{l},N,{k}] B[{l},{k},{n}]\"\n",
        "plt.title(title)\n",
        "plt.rcParams[\"font.size\"] = \"17\"\n",
        "plt.grid()\n",
        "colors = [\"red\", \"blue\"]\n",
        "labels = [\"custom_topk_bmm\", \"torch.bmm -> torch.topk\"]\n",
        "for i, res in enumerate([custom_res, cublass_res]):\n",
        "  res_x = list(res.keys())\n",
        "  res_y = list(res.values())\n",
        "  # plt.plot(\n",
        "  # plt.loglog(\n",
        "  plt.semilogx(\n",
        "    res_x,\n",
        "    res_y,\n",
        "    color=colors[i],\n",
        "    label=labels[i],\n",
        "  )\n",
        "  # plt.plot(res_x, res_y, colors[i])\n",
        "plt.legend()\n",
        "plt.savefig(\"imgs/topk_bmm_\" + title + \"_memory_semilogx\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "l=1  m=512  n=1024  k=1024\n",
            "l=1  m=1024  n=1024  k=1024\n",
            "l=1  m=2048  n=1024  k=1024\n",
            "l=1  m=4096  n=1024  k=1024\n",
            "l=1  m=8192  n=1024  k=1024\n",
            "l=1  m=16384  n=1024  k=1024\n",
            "l=1  m=32768  n=1024  k=1024\n",
            "l=1  m=65536  n=1024  k=1024\n",
            "l=1  m=131072  n=1024  k=1024\n",
            "l=1  m=262144  n=1024  k=1024\n",
            "l=1  m=524288  n=1024  k=1024\n",
            "l=1  m=1048576  n=1024  k=1024\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeSM15FnoIDR",
        "cellView": "form"
      },
      "source": [
        "#@title Grid test TopkBMM (Runtime)\n",
        "ls = [1]\n",
        "ms = [256* 2**i for i in range(1, 13)]\n",
        "ns = [1024]\n",
        "\n",
        "ks = [1024]\n",
        "mode=\"nn\"\n",
        "\n",
        "custom_res = dict()\n",
        "cublass_res = dict()\n",
        "for l in ls:\n",
        "  for n in ns:\n",
        "    for m in ms:\n",
        "      for k in ks:     \n",
        "        res = test_topk_bmm(\n",
        "          l, m, n, k,\n",
        "          mode=mode, n_iter=15, dim=1,\n",
        "          n_candidates = 128,\n",
        "        )\n",
        "        cublass_res[m] = res[0]*1e3\n",
        "        custom_res[m] = res[1]*1e3\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10) )\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"N\", fontsize=17)\n",
        "plt.ylabel(\"milliseconds\", fontsize=17)\n",
        "title = f\"A[{l},N,{k}] B[{l},{k},{n}]\"\n",
        "plt.title(title)\n",
        "plt.rcParams[\"font.size\"] = \"17\"\n",
        "plt.grid()\n",
        "colors = [\"red\", \"blue\"]\n",
        "labels = [\"custom_topk_bmm\", \"torch.bmm -> torch.topk\"]\n",
        "for i, res in enumerate([custom_res, cublass_res]):\n",
        "  res_x = list(res.keys())\n",
        "  res_y = list(res.values())\n",
        "  # plt.plot(\n",
        "  # plt.loglog(\n",
        "  plt.semilogx(\n",
        "    res_x,\n",
        "    res_y,\n",
        "    color=colors[i],\n",
        "    label=labels[i],\n",
        "  )\n",
        "  # plt.plot(res_x, res_y, colors[i])\n",
        "plt.legend()\n",
        "plt.savefig(\"imgs/topk_bmm_\" + title + \"_semilogx\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}