{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_topkbmm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU4BcFUE5E0J"
      },
      "source": [
        "#!pip install --upgrade cupy-cuda112==8.5.0"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75DR78cKpM3m"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "import math\n",
        "from time import time"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "12phuKJIFY9A"
      },
      "source": [
        "#@title CustomKernel\n",
        "import cupy as cp\n",
        "import torch\n",
        "\n",
        "@cp.util.memoize(for_each_device=True)\n",
        "def cunnex(func_name, func_body):\n",
        "  return cp.cuda.compile_with_cache(func_body).get_function(func_name)\n",
        "\n",
        "class Stream:\n",
        "  def __init__(self, ptr):\n",
        "    self.ptr = ptr\n",
        "  \n",
        "class CustomKernel:\n",
        "  def __init__(self):\n",
        "    self._use_torch_in_cupy_malloc()\n",
        "    self.stream = Stream(torch.cuda.current_stream().cuda_stream)\n",
        "\n",
        "  @staticmethod\n",
        "  def _torch_alloc(size):\n",
        "    device = cp.cuda.Device().id\n",
        "    tensor = torch.empty(size, dtype=torch.uint8, device=device)\n",
        "    return cp.cuda.MemoryPointer(\n",
        "        cp.cuda.UnownedMemory(tensor.data_ptr(), size, tensor), 0)\n",
        "\n",
        "  def _use_torch_in_cupy_malloc(self):\n",
        "    cp.cuda.set_allocator(self._torch_alloc)\n",
        "\n",
        "  def _compile_kernel_str(\n",
        "      self,\n",
        "      kernel,\n",
        "      name,\n",
        "      options=(),\n",
        "      backend=\"nvrtc\",\n",
        "      max_dynamic_smem=None\n",
        "    ):\n",
        "    fn = cp.RawKernel(\n",
        "      kernel,\n",
        "      name,\n",
        "      options=options,\n",
        "      backend=backend,\n",
        "    )\n",
        "    if max_dynamic_smem:\n",
        "      fn.max_dynamic_shared_size_bytes = max_dynamic_smem\n",
        "    return fn"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVay8JGQU_mr",
        "cellView": "form"
      },
      "source": [
        "#@title BitonicSort Kernel\n",
        "\n",
        "kernel = \"\"\"\n",
        "typedef long long ll_t;\n",
        "#define isnan(x) ( x != x )\n",
        "\n",
        "#if (__CUDA_ARCH__ < 700)\n",
        "__device__ void __nanosleep(unsigned int ns){\n",
        "  clock_t start_clock = clock();\n",
        "  clock_t clock_offset = 0;\n",
        "  while (clock_offset < ns)\n",
        "  {\n",
        "    clock_offset = clock() - start_clock;\n",
        "  }\n",
        "}\n",
        "#endif \n",
        "\n",
        "/*\n",
        "mutex lock code from:\n",
        "https://stackoverflow.com/questions/18963293/cuda-atomics-change-flag/18968893#18968893\n",
        "*/\n",
        "\n",
        "__device__ void mutex_lock_v2(\n",
        "  unsigned int *mutex\n",
        ") {\n",
        "  unsigned int ns = 8;\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0){\n",
        "    while (atomicCAS(mutex, 0, 1) == 1) {\n",
        "      __nanosleep(ns);\n",
        "      if (ns < 256) {\n",
        "        ns *= 2;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_lock(\n",
        "  unsigned int *mutex,\n",
        "  unsigned int blockMutex[1]\n",
        ") {\n",
        "  unsigned int ns = 8;\n",
        "  float old_value;\n",
        "  if (threadIdx.x == 0){\n",
        "    old_value = atomicCAS(mutex, 0, 1);\n",
        "    blockMutex[0] = old_value;\n",
        "  }\n",
        "  __syncthreads();\n",
        "  old_value = blockMutex[0];\n",
        "  while (old_value == 1) {\n",
        "    __nanosleep(ns);\n",
        "    if (ns < 256) {\n",
        "      ns *= 2;\n",
        "    }\n",
        "\n",
        "    if (threadIdx.x == 0){\n",
        "      old_value = atomicCAS(mutex, 0, 1);\n",
        "      blockMutex[0] = old_value;\n",
        "    }\n",
        "    __syncthreads();\n",
        "    old_value = blockMutex[0];\n",
        "    __syncthreads();\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock_v2(unsigned int *mutex) {\n",
        "  __threadfence();\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicExch(mutex, 0);\n",
        "    __threadfence();\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock(unsigned int *mutex) {\n",
        "  atomicExch(mutex, 0);\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ unsigned int bfe(\n",
        "  unsigned int source,\n",
        "  unsigned int bitIndex\n",
        ") {\n",
        "  unsigned int bit;\n",
        "  asm volatile(\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bit) : \"r\"((unsigned int) source), \"r\"(bitIndex), \"r\"(1));\n",
        "  return bit;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void warpComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction\n",
        "){\n",
        "  const float other_value = __shfl_xor_sync(0xFFFFFFFF, value, stride);\n",
        "  const float other_index = __shfl_xor_sync(0xFFFFFFFF, index, stride);\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void blockComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction,\n",
        "  const int laneID,\n",
        "  float valSM[128],\n",
        "  float idxSM[128]\n",
        "){\n",
        "  valSM[laneID] = value;\n",
        "  idxSM[laneID] = index;\n",
        "  __syncthreads();\n",
        "\n",
        "  float other_value = valSM[laneID ^ stride];\n",
        "  float other_index = idxSM[laneID ^ stride];\n",
        "  __syncthreads();\n",
        "\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort256(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  float valSM[128],\n",
        "  float idxSM[128],\n",
        "  int gStartx, int Q\n",
        "){\n",
        "  float other_value = values[threadIdx.x];\n",
        "  float other_index = indices[threadIdx.x] - gStartx;\n",
        "  \n",
        "  bool condition = value > other_value == 0;\n",
        "  if (condition){\n",
        "    float temp_value = value;\n",
        "    float temp_index = index;\n",
        "    value = other_value;\n",
        "    index = other_index;\n",
        "    other_value = temp_value;\n",
        "    other_index = temp_index;\n",
        "  }\n",
        "\n",
        "  int laneID = threadIdx.x % 128;\n",
        "  int i = 7;\n",
        "  for (int j = 6; j >= 0; j--){\n",
        "    unsigned int direction = bfe(laneID, 8) ^ bfe(laneID, j);\n",
        "    int stride = pow(2, j);\n",
        "    if (stride < 32){\n",
        "      warpComparator(value, index, stride, !direction);\n",
        "    } else {\n",
        "      blockComparator(value, index, stride, !direction, laneID, valSM, idxSM);\n",
        "    }\n",
        "  }\n",
        "\n",
        "  if (threadIdx.x < Q){\n",
        "    values[threadIdx.x] = value;\n",
        "    indices[threadIdx.x] = index + gStartx;\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  float valSM[128],\n",
        "  float idxSM[128]\n",
        ") {\n",
        "  unsigned int laneID = threadIdx.x % 128;\n",
        "  for (int i=0; i < 7; i++){\n",
        "    for (int j=i; j >= 0; j--){\n",
        "      unsigned int direction = bfe(laneID, i + 1) ^ bfe(laneID, j);\n",
        "      int stride = pow(2, j);\n",
        "      // if (i==6 && j==0) break;\n",
        "      if (stride < 32){\n",
        "        warpComparator(value, index, stride, direction);\n",
        "      } else {\n",
        "        blockComparator(value, index, stride, direction, laneID, valSM, idxSM);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bitonic_sort(\n",
        "   const float* __restrict__ arr,\n",
        "   float* values,\n",
        "   ll_t* indices,\n",
        "   unsigned int* mutex,\n",
        "   int L, int Q\n",
        "){\n",
        "  int gStartx = blockIdx.x * 128;\n",
        "  int tid = threadIdx.x;\n",
        "  __shared__ float valSM[128];\n",
        "  __shared__ float idxSM[128];\n",
        "  \n",
        "  float value;\n",
        "  float index;\n",
        "  int iL = gStartx + tid;\n",
        "  if (iL < L){\n",
        "    value = arr[iL];\n",
        "    index = tid;\n",
        "  } else {\n",
        "    value = -INFINITY;\n",
        "  }\n",
        "  \n",
        "  bitonicSort(value, index, valSM, idxSM);\n",
        "\n",
        "  __shared__ unsigned int blockMutex[1];\n",
        "  mutex_lock_v2(mutex);\n",
        "\n",
        "  bitonicSort256(\n",
        "    value, index, values, indices,\n",
        "    valSM, idxSM, gStartx, Q\n",
        "  );\n",
        "  \n",
        "  mutex_unlock_v2(mutex);\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "with open(\"BitonicSort.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmlIFoKnSRs0",
        "cellView": "form"
      },
      "source": [
        "#@title BitonicSort\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class BitonicSort(CustomKernel): \n",
        "  def __init__(self):\n",
        "    super(BitonicSort, self).__init__()\n",
        "    \n",
        "    with open(\"BitonicSort.cu\",'r') as f: ###\n",
        "      self.kernel = f.read()\n",
        "    \n",
        "    self._fn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bitonic_sort\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "\n",
        "  def __call__(self, arr):\n",
        "    l = arr.shape[0]\n",
        "    q = 128\n",
        "    threads_per_block = (128,)\n",
        "    blocks_per_grid = ( math.ceil(l/128), )\n",
        "    values = torch.empty(128, device=\"cuda:0\", dtype=torch.float)\n",
        "    values.fill_(float(\"-inf\"))\n",
        "    indices = torch.empty(128, device=\"cuda:0\", dtype=torch.long)\n",
        "    mutex = torch.zeros(1, device=\"cuda:0\", dtype=torch.int)\n",
        "\n",
        "    self._fn(\n",
        "      grid = blocks_per_grid,\n",
        "      block = threads_per_block,\n",
        "      args = [\n",
        "        arr.data_ptr(),\n",
        "        values.data_ptr(),\n",
        "        indices.data_ptr(),\n",
        "        mutex.data_ptr(),\n",
        "        l, q\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    print(mutex)\n",
        "    return values, indices\n",
        "\n",
        "# x = torch.randn(128*1024, device=\"cuda:0\")\n",
        "# bitonic_sort = BitonicSort()\n",
        "\n",
        "# v1, i1 = torch.topk(x, k=128)\n",
        "# v2, i2 = bitonic_sort(x)\n",
        "# print(i1)\n",
        "# print(i2)\n",
        "\n",
        "# # plt.plot(x.cpu())\n",
        "# # plt.show()\n",
        "\n",
        "# # print(v2)\n",
        "# plt.plot(v1.cpu())\n",
        "# plt.show()\n",
        "\n",
        "# plt.plot(v2.cpu())\n",
        "# plt.show()\n",
        "\n",
        "# # x2 = x.sort(dim=0, descending=True)[0]\n",
        "# # plt.plot(x2.cpu())\n",
        "# # plt.show()\n",
        "\n",
        "\n",
        "# val_dif = (v1 - v2).abs()\n",
        "# idx_dif = (i1 != i2)\n",
        "\n",
        "# print(\"val error\", val_dif.sum())\n",
        "# print(\"idx error\", idx_dif.sum())"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nn_2LdNS6ibb",
        "cellView": "form"
      },
      "source": [
        "#@title TopkBMM Kernel\n",
        "kernel = \"\"\"\n",
        "#define _VOLATILE_  \n",
        "\n",
        "#define likely(x)      __builtin_expect(!!(x), 1)\n",
        "#define unlikely(x)    __builtin_expect(!!(x), 0)\n",
        "#define load(x)        __ldcg(x)\n",
        "#define store(x, value) __stcs(x, value)\n",
        "#define isnan(x) ( x != x )\n",
        "#if (__CUDA_ARCH__ < 700)\n",
        "__device__ void __nanosleep(unsigned int ns){\n",
        "  clock_t start_clock = clock();\n",
        "  clock_t clock_offset = 0;\n",
        "  while (clock_offset < ns)\n",
        "  {\n",
        "    clock_offset = clock() - start_clock;\n",
        "  }\n",
        "}\n",
        "#endif \n",
        "\n",
        "typedef long long ll_t;\n",
        "typedef unsigned long long ull_t;\n",
        "\n",
        "typedef struct __builtin_align__(32) {\n",
        "  float s0, s1, s2, s3, s4, s5, s6, s7;\n",
        "} _float8;\n",
        "\n",
        "typedef union {\n",
        "  _float8 f8;\n",
        "  float val[8];\n",
        "} float8;\n",
        "\n",
        "__device__ void mutex_lock(\n",
        "  unsigned int *mutex\n",
        ") {\n",
        "  unsigned int ns = 8;\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0 ){\n",
        "    while (atomicCAS(mutex, 0, 1) == 1) {\n",
        "      __nanosleep(ns);\n",
        "      if (ns < 256) {\n",
        "        ns *= 2;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_lock_noop(\n",
        ") {\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock(\n",
        "  unsigned int *mutex\n",
        ") {\n",
        "  __threadfence();\n",
        "  __syncthreads();\n",
        "  if (threadIdx.x == 0){\n",
        "    atomicExch(mutex, 0);\n",
        "    __threadfence();\n",
        "  }\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ void mutex_unlock_noop(){\n",
        "  __syncthreads();\n",
        "  __syncthreads();\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ unsigned int bfe(\n",
        "  unsigned int source,\n",
        "  unsigned int bitIndex\n",
        ") {\n",
        "  unsigned int bit;\n",
        "  asm volatile(\"bfe.u32 %0, %1, %2, %3;\" : \"=r\"(bit) : \"r\"((unsigned int) source), \"r\"(bitIndex), \"r\"(1));\n",
        "  return bit;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void warpComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction\n",
        "){\n",
        "  const float other_value = __shfl_xor_sync(0xFFFFFFFF, value, stride);\n",
        "  const float other_index = __shfl_xor_sync(0xFFFFFFFF, index, stride);\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ __forceinline__ void blockComparator(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  const int stride,\n",
        "  const int direction,\n",
        "  const int laneID,\n",
        "  _VOLATILE_ float valSM[128+4],\n",
        "  _VOLATILE_ float idxSM[128+4]\n",
        "){\n",
        "  valSM[laneID] = value;\n",
        "  idxSM[laneID] = index;\n",
        "  __syncthreads();\n",
        "\n",
        "  __syncthreads();\n",
        "  float other_value = valSM[laneID ^ stride];\n",
        "  float other_index = idxSM[laneID ^ stride];\n",
        "  __syncthreads();\n",
        "\n",
        "  bool condition = value < other_value == direction;\n",
        "  index = condition ? other_index : index;\n",
        "  value = condition ? other_value : value;\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort128(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  _VOLATILE_ float valSM[128+4],\n",
        "  _VOLATILE_ float idxSM[128+4]\n",
        ") {\n",
        "  unsigned int laneID = threadIdx.x % 128;\n",
        "  warpComparator(value, index, 1, bfe(laneID, 1) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 2, bfe(laneID, 2) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 2) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 4, bfe(laneID, 3) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 3) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 3) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 8, bfe(laneID, 4) ^ bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 4) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 4) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 4) ^ bfe(laneID, 0));\n",
        "\n",
        "  warpComparator(value, index, 16, bfe(laneID, 5) ^ bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, bfe(laneID, 5) ^ bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 5) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 5) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 5) ^ bfe(laneID, 0));\n",
        "\n",
        "  blockComparator(value, index, 32, bfe(laneID, 6) ^ bfe(laneID, 5), laneID, valSM, idxSM);\n",
        "  warpComparator(value, index, 16, bfe(laneID, 6) ^ bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, bfe(laneID, 6) ^ bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 6) ^ bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 6) ^ bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 6) ^ bfe(laneID, 0));\n",
        "\n",
        "  blockComparator(value, index, 64, bfe(laneID, 6), laneID, valSM, idxSM);\n",
        "  blockComparator(value, index, 32, bfe(laneID, 5), laneID, valSM, idxSM);\n",
        "  warpComparator(value, index, 16, bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, bfe(laneID, 0));\n",
        "}\n",
        "\n",
        "__device__ void bitonicSort256(\n",
        "  float &value,\n",
        "  float &index,\n",
        "  float* gValue,\n",
        "  ll_t* gIndex,\n",
        "  float valSM[128+4],\n",
        "  float idxSM[128+4],\n",
        "  int Q\n",
        "){\n",
        "  int laneID = threadIdx.x % 128;\n",
        "  float other_value = gValue[0];\n",
        "  float other_index = gIndex[0];\n",
        "  \n",
        "  bool condition = value > other_value == 0;\n",
        "  if (condition){\n",
        "    //float temp_value = value;\n",
        "    //float temp_index = index;\n",
        "    value = value + other_value;\n",
        "    index = index + other_index;\n",
        "    other_value = value - other_value;\n",
        "    other_index = index - other_index;\n",
        "    value = value - other_value;\n",
        "    index = index - other_index;\n",
        "  }\n",
        "\n",
        "  blockComparator(value, index, 64, !bfe(laneID, 6), laneID, valSM, idxSM);\n",
        "  blockComparator(value, index, 32, !bfe(laneID, 5), laneID, valSM, idxSM);\n",
        "  warpComparator(value, index, 16, !bfe(laneID, 4));\n",
        "  warpComparator(value, index, 8, !bfe(laneID, 3));\n",
        "  warpComparator(value, index, 4, !bfe(laneID, 2));\n",
        "  warpComparator(value, index, 2, !bfe(laneID, 1));\n",
        "  warpComparator(value, index, 1, !bfe(laneID, 0));\n",
        "\n",
        "  if ( laneID < Q){\n",
        "    gValue[0] = value;\n",
        "    gIndex[0] = index;\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void topk_dim_1(\n",
        "  float8 cCache[8],\n",
        "  _VOLATILE_ float valSM[16][128+4],\n",
        "  _VOLATILE_ float idxSM[16][128+4],\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int gStartx, int gStarty, int bid,\n",
        "  int M, int N, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int hx = tid % 128;\n",
        "  int hy = tid / 128;\n",
        "  #pragma unroll\n",
        "  for (int ni=0; ni<8; ni++){\n",
        "    if (gStartx + vx*8 + ni >= N)\n",
        "      break;\n",
        "\n",
        "    // Store cCache to cSM\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      int iM = gStarty + vy*8 + mi;\n",
        "      if (likely(iM < M)){\n",
        "        valSM[vx][vy*8 + mi] = cCache[mi].val[ni];\n",
        "        idxSM[vx][vy*8 + mi] = iM;\n",
        "      } else {\n",
        "        valSM[vx][vy*8 + mi] = -INFINITY;\n",
        "        idxSM[vx][vy*8 + mi] = iM;\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Load from cSM to cCache\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<8; i++){\n",
        "      float value = valSM[hy*8 + i][hx];\n",
        "      float index = idxSM[hy*8 + i][hx];\n",
        "      bitonicSort128(\n",
        "        value, index,\n",
        "        valSM[hy*8 + i], idxSM[hy*8 + i]\n",
        "      );\n",
        "      int iN = gStartx + (hy*8 + i)*8 + ni;\n",
        "      mutex_lock( &mutex[(bid)*N + iN] );\n",
        "      bitonicSort256(\n",
        "        value, index, \n",
        "        &values[(bid)*N*Q + iN*Q + hx],\n",
        "        &indices[(bid)*N*Q + iN*Q + hx], \n",
        "        valSM[hy*8+i], idxSM[hy*8+i],\n",
        "        Q\n",
        "      );\n",
        "      mutex_unlock( &mutex[(bid)*N + iN] );\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void topk_dim_2(\n",
        "  float8 cCache[8],\n",
        "  _VOLATILE_ float valSM[16][128+4],\n",
        "  _VOLATILE_ float idxSM[16][128+4],\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int gStartx, int gStarty, int bid,\n",
        "  int M, int N, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int hx = tid % 128;\n",
        "  int hy = tid / 128;\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    if (gStarty + vy*8 + mi >= M)\n",
        "      break;\n",
        "\n",
        "    // Store cCache to cSM\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      int iN = gStartx + vx*8 + ni;\n",
        "      if (likely(iN < N)){\n",
        "        valSM[vy][vx*8 + ni] = cCache[mi].val[ni];\n",
        "        idxSM[vy][vx*8 + ni] = iN;\n",
        "      } else {\n",
        "        valSM[vy][vx*8 + ni] = -INFINITY;\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "    // Load from cSM to cCache\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<8; i++){\n",
        "      float value = valSM[hy*8 + i][hx];\n",
        "      float index = idxSM[hy*8 + i][hx];\n",
        "      bitonicSort128(\n",
        "        value, index,\n",
        "        valSM[hy*8 + i], idxSM[hy*8 + i]\n",
        "      );\n",
        "      int iM = gStarty + (hy*8 + i)*8 + mi;\n",
        "      mutex_lock( &mutex[(bid)*M + iM] );\n",
        "      bitonicSort256(\n",
        "        value, index, \n",
        "        &values[(bid)*M*Q + iM*Q + hx],\n",
        "        &indices[(bid)*M*Q + iM*Q + hx], \n",
        "        valSM[hy*8+i], idxSM[hy*8+i],\n",
        "        Q\n",
        "      );\n",
        "      mutex_unlock( &mutex[(bid)*M + iM] );\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void init_cCache(\n",
        "  float8 cCache[8]\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    #pragma unroll\n",
        "    for (int j=0; j<8; j++){\n",
        "      cCache[i].val[j] = 0.f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v4(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache1[8];\n",
        "  float aCache2[8];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    aCache1[mi] = aSM[0][8*vy + mi];\n",
        "  }\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    int is_odd = ki & 1;\n",
        "    if (is_odd == 0){\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache2[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache1[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    } else {\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache1[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache2[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v3(\n",
        "  _VOLATILE_ float aSM[16][128+4],\n",
        "  _VOLATILE_ float bSM[16][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache[8];\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<16; ki++){\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      aCache[mi] = aSM[ki][8*vy + mi];\n",
        "    }\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "      #pragma unroll\n",
        "      for (int mi=0; mi<8; mi++){\n",
        "        float a = aCache[mi];\n",
        "        cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// Unsafe\n",
        "__device__ void write_c(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    int iM = gStarty + vy*8 + i;\n",
        "    if (likely(iM < M)){\n",
        "      int iN_start = gStartx + vx*8;\n",
        "      reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      /*\n",
        "      if (likely(iN_start + 7 < N)){\n",
        "        reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      } else {\n",
        "        #pragma unroll\n",
        "        for (int j=0; j<8; j++){\n",
        "          int iN = iN_start + j;\n",
        "          if (iN < N){\n",
        "            C[(bid)*M*N + (iM)*N + (iN)] = cCache[i].val[j];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      */\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void write_c_v3(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  __shared__ volatile float cSM[16][128];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    int iM = gStarty + vy*8 + mi;\n",
        "    // Store 1 row from cCache to cSM\n",
        "    if (iM < M){\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        cSM[vy][vx*8 + ni] = cCache[mi].val[ni];\n",
        "      }\n",
        "      // Store to C\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        int iN = gStartx + 16*ni + vx;\n",
        "        if (iN < N){\n",
        "          float cVal = cSM[vy][16*ni + vx];\n",
        "          store(C+(bid)*M*N + (iM)*N + (iN), cVal);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  } \n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_tn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ values,\n",
        "  ll_t* __restrict__ indices,\n",
        "  int M, int N, int K, int DIM\n",
        "){\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_nt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ values,\n",
        "  ll_t* __restrict__ indices,\n",
        "  int M, int N, int K, int DIM\n",
        "){\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_nn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* values,\n",
        "  ll_t* indices,\n",
        "  unsigned int* mutex,\n",
        "  int M, int N, int K, int DIM, int Q\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM[16][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM[16][128+4];\n",
        "\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (likely(iM < _M_)){\n",
        "      if (likely(dx < _K_)){\n",
        "        aBuffer1[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (dx));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(dx+8 < _K_)){\n",
        "        aBuffer2[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (dx+8));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(wy < _K_)){\n",
        "        bBuffer1[i] = load(B + (bid)*_N_*_K_ + (wy)*_N_ + (iN));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(wy+8 < _K_)){\n",
        "        bBuffer2[i] = load(B + (bid)*_N_*_K_ + (wy+8)*_N_ + (iN));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    // Index on K axis of A and B\n",
        "    int iKA = gStartk + 16 + dx;\n",
        "    int iKB = gStartk + 16 + wy;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<4; i++){\n",
        "      // Store buffered tiles into shared memory\n",
        "      aSM[dx][dy+i*32] = aBuffer1[i];\n",
        "      bSM[wy][wx+i*32+i] = bBuffer1[i];\n",
        "      aSM[8 + dx][dy+i*32] = aBuffer2[i];\n",
        "      bSM[8 + wy][wx+i*32+i] = bBuffer2[i];\n",
        "\n",
        "      // Start loading next 16*128 tile of A and B to buffer1 and buffer2.\n",
        "      // Don't load anything on the last iteration.\n",
        "      // Loading from global memory will not block thread_matmul\n",
        "      if (likely(itr < nIt - 1)){\n",
        "        int iM = gStarty + i*32 + dy;\n",
        "        int iN = gStartx + i*32 + wx;\n",
        "        \n",
        "        if (likely(iM < _M_)){\n",
        "          if (likely(iKA < _K_)){\n",
        "            aBuffer1[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (iKA));\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "          }\n",
        "          if (likely(iKA+8 < _K_)){\n",
        "            aBuffer2[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (iKA+8));\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "\n",
        "        if (likely(iN < _N_)){\n",
        "          if (likely(iKB < _K_)){\n",
        "            bBuffer1[i] = load(B + (bid)*_N_*_K_ + (iKB)*_N_ + (iN));\n",
        "          } else {\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "          if (likely(iKB+8 < _K_)){\n",
        "            bBuffer2[i] = load(B + (bid)*_N_*_K_ + (iKB+8)*_N_ + (iN));\n",
        "          } else {\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    thread_matmul_v3(aSM, bSM, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  // TopK sort along DIM\n",
        "  if (DIM == 1){\n",
        "    topk_dim_1(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  } else if (DIM == 2){\n",
        "    topk_dim_2(\n",
        "      cCache, aSM, bSM,\n",
        "      values, indices, mutex,\n",
        "      gStartx, gStarty, bid, M, N, Q);\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void topk_bmm_tt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ values,\n",
        "  ll_t* __restrict__ indices,\n",
        "  int M, int N, int K, int DIM\n",
        "){\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"TopkBMMCUDA.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nm88_WSG7ig1",
        "cellView": "form"
      },
      "source": [
        "#@title TopkBMM\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class TopkBMMCUDA(CustomKernel): \n",
        "  def __init__(self, m=None, n=None, k=None, patch_m=4, patch_n=4):\n",
        "    super(TopkBMMCUDA, self).__init__()\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = k\n",
        "    self.patch_m = patch_m\n",
        "    self.patch_n = patch_n\n",
        "    \n",
        "    with open(\"TopkBMMCUDA.cu\",'r') as f: ###\n",
        "      self.kernel = f.read()\n",
        "      \n",
        "    self.kernel = (self.kernel\n",
        "      .replace(\"_M_\", str(m) if m else \"M\")\n",
        "      .replace(\"_N_\", str(n) if n else \"N\")\n",
        "      .replace(\"_K_\", str(k) if k else \"K\")\n",
        "      .replace(\"_PM_\", str(self.patch_m))\n",
        "      .replace(\"_PN_\", str(self.patch_n))\n",
        "    )\n",
        "    \n",
        "    self._fn_tt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_tt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_nn\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "    # print(self._fn_nn.attributes)\n",
        "    self._fn_tn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_tn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"topk_bmm_nt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "\n",
        "  def _call_nn(self, A, B, n_candidates, dim):\n",
        "    \"\"\"\n",
        "      Performs C = A @ B\n",
        "      A: shape = [l, m, k]\n",
        "      B: shape = [l, k, n]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[1]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "    assert dim in [1, 2]\n",
        "    assert 0 < n_candidates <= 128\n",
        "    \n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "    if dim == 1:\n",
        "      values = torch.empty([l, n, n_candidates], device=\"cuda:0\", dtype=A.dtype)\n",
        "      indices = torch.empty([l, n, n_candidates], device=\"cuda:0\", dtype=torch.int64)\n",
        "      mutex = torch.zeros([l, n], device=\"cuda:0\", dtype=torch.int32)\n",
        "    elif dim == 2:\n",
        "      values = torch.empty([l, m, n_candidates], device=\"cuda:0\", dtype=A.dtype)\n",
        "      indices = torch.empty([l, m, n_candidates], device=\"cuda:0\", dtype=torch.int64)\n",
        "      mutex = torch.zeros([l, m], device=\"cuda:0\", dtype=torch.int32)\n",
        "    values.fill_(float(\"-inf\"))\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_nn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        values.data_ptr(),\n",
        "        indices.data_ptr(),\n",
        "        mutex.data_ptr(),\n",
        "        m, n, k, dim, n_candidates\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return values, indices\n",
        "\n",
        "  def _call_tt(self, A, B, n_candidates, dim):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _call_tn(self, A, B, n_candidates, dim):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _call_nt(self, A, B, n_candidates, dim):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def __call__(self, A, B, k=128, dim=1, mode=\"nn\"):\n",
        "    \"\"\"\n",
        "      Performs C = min(f(A) @ g(B)), argmin(f(A) @ g(B))\n",
        "      A: torch.Tensor, shape : [l, m, k] or [l, k, m]\n",
        "      B: torch.Tensor, shape : [l, n, k] or [l, k, n]\n",
        "      returns C: torch.Tensor, shape : [l, m, n]\n",
        "      mode: str, default: \"nn\"\n",
        "      Notes:\n",
        "        f() and g() are determined by mode\n",
        "        \"nn\" --> A @ B\n",
        "        \"tt\" --> A.T @ B.T\n",
        "        \"nt\" --> A @ B.T\n",
        "        \"tn\" --> A.T @ B\n",
        "    \"\"\"\n",
        "    assert len(A.shape) == len(B.shape)\n",
        "    A = A.contiguous()\n",
        "    B = B.contiguous()\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      A2 = A[None]\n",
        "      B2 = B[None]\n",
        "    elif len(A.shape) == 3 and len(B.shape) == 3:\n",
        "      A2 = A\n",
        "      B2 = B\n",
        "    else:\n",
        "      raise ValueError(\"shape of A and B need to be 2d or 3d\")\n",
        "\n",
        "    if mode == \"nn\":\n",
        "      values, indices = self._call_nn(A2, B2, k, dim)\n",
        "    elif mode == \"tt\":\n",
        "      values, indices = self._call_tt(A2, B2, k, dim)\n",
        "    elif mode == \"tn\":\n",
        "      values, indices = self._call_tn(A2, B2, k, dim)\n",
        "    elif mode == \"nt\":\n",
        "      values, indices = self._call_nt(A2, B2, k, dim)\n",
        "\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      indices = indices[0]\n",
        "      values = values[0]\n",
        "\n",
        "    return values, indices"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDuqX_JrO-50",
        "cellView": "form"
      },
      "source": [
        "#@title BMMv2.5 Kernel\n",
        "kernel = \"\"\"\n",
        "#define _VOLATILE_  \n",
        "\n",
        "#define likely(x)      __builtin_expect(!!(x), 1)\n",
        "#define unlikely(x)    __builtin_expect(!!(x), 0)\n",
        "#define load(x)        __ldcg(x)\n",
        "#define store(x, value) __stcs(x, value)\n",
        "\n",
        "typedef long long ll_t;\n",
        "typedef unsigned long long ull_t;\n",
        "\n",
        "typedef struct __builtin_align__(32) {\n",
        "  float s0, s1, s2, s3, s4, s5, s6, s7;\n",
        "} _float8;\n",
        "\n",
        "typedef union {\n",
        "  _float8 f8;\n",
        "  float val[8];\n",
        "} float8;\n",
        "\n",
        "__device__ void init_cCache(\n",
        "  float8 cCache[8]\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    #pragma unroll\n",
        "    for (int j=0; j<8; j++){\n",
        "      cCache[i].val[j] = 0.f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v4(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache1[8];\n",
        "  float aCache2[8];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    aCache1[mi] = aSM[0][8*vy + mi];\n",
        "  }\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    int is_odd = ki & 1;\n",
        "    if (is_odd == 0){\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache2[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache1[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    } else {\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache1[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache2[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v3(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache[8];\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      aCache[mi] = aSM[ki][8*vy + mi];\n",
        "    }\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "      #pragma unroll\n",
        "      for (int mi=0; mi<8; mi++){\n",
        "        float a = aCache[mi];\n",
        "        cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// Unsafe\n",
        "__device__ void write_c(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    int iM = gStarty + vy*8 + i;\n",
        "    if (likely(iM < M)){\n",
        "      int iN_start = gStartx + vx*8;\n",
        "      reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      /*\n",
        "      if (likely(iN_start + 7 < N)){\n",
        "        reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      } else {\n",
        "        #pragma unroll\n",
        "        for (int j=0; j<8; j++){\n",
        "          int iN = iN_start + j;\n",
        "          if (iN < N){\n",
        "            C[(bid)*M*N + (iM)*N + (iN)] = cCache[i].val[j];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      */\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void write_c_v3(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  __shared__ volatile float cSM[16][128];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    int iM = gStarty + vy*8 + mi;\n",
        "    // Store 1 row from cCache to cSM\n",
        "    if (iM < M){\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        cSM[vy][vx*8 + ni] = cCache[mi].val[ni];\n",
        "      }\n",
        "      // Store to C\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        int iN = gStartx + 16*ni + vx;\n",
        "        if (iN < N){\n",
        "          float cVal = cSM[vy][16*ni + vx];\n",
        "          store(C+(bid)*M*N + (iM)*N + (iN), cVal);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  } \n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float aSM2[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM2[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (likely(iM < _M_)){\n",
        "      if (likely(dx < _K_)){\n",
        "        aBuffer1[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (dx));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(dx+8 < _K_)){\n",
        "        aBuffer2[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (dx+8));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(wy < _K_)){\n",
        "        bBuffer1[i] = load(B + (bid)*_N_*_K_ + (wy)*_N_ + (iN));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(wy+8 < _K_)){\n",
        "        bBuffer2[i] = load(B + (bid)*_N_*_K_ + (wy+8)*_N_ + (iN));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    // Index on K axis of A and B\n",
        "    int iKA = gStartk + 16 + dx;\n",
        "    int iKB = gStartk + 16 + wy;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<4; i++){\n",
        "      // Store buffered tiles into shared memory\n",
        "      aSM1[dx][dy+i*32] = aBuffer1[i];\n",
        "      bSM1[wy][wx+i*32+i] = bBuffer1[i];\n",
        "      aSM2[dx][dy+i*32] = aBuffer2[i];\n",
        "      bSM2[wy][wx+i*32+i] = bBuffer2[i];\n",
        "\n",
        "      // Start loading next 16*128 tile of A and B to buffer1 and buffer2.\n",
        "      // Don't load anything on the last iteration.\n",
        "      // Loading from global memory will not block thread_matmul\n",
        "      if (likely(itr < nIt - 1)){\n",
        "        int iM = gStarty + i*32 + dy;\n",
        "        int iN = gStartx + i*32 + wx;\n",
        "        \n",
        "        if (likely(iM < _M_)){\n",
        "          if (likely(iKA < _K_)){\n",
        "            aBuffer1[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (iKA));\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "          }\n",
        "          if (likely(iKA+8 < _K_)){\n",
        "            aBuffer2[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (iKA+8));\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "\n",
        "        if (likely(iN < _N_)){\n",
        "          if (likely(iKB < _K_)){\n",
        "            bBuffer1[i] = load(B + (bid)*_N_*_K_ + (iKB)*_N_ + (iN));\n",
        "          } else {\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "          if (likely(iKB+8 < _K_)){\n",
        "            bBuffer2[i] = load(B + (bid)*_N_*_K_ + (iKB+8)*_N_ + (iN));\n",
        "          } else {\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    // Each thread computes 8 x 8 matrix multiplication\n",
        "    // Accumulate intermediate results in cCache\n",
        "    // aSM1, bSM1, aSM2, bSM2 are consumed\n",
        "    thread_matmul_v3(aSM1, bSM1, cCache, vx, vy);\n",
        "    thread_matmul_v3(aSM2, bSM2, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "  \n",
        "  // At the end of main loop, store cCache to C\n",
        "  //write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "  write_c_v3(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "\n",
        "  //C[bIdxY * N + bIdxX] = gStarty;\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"BMMCUDAv2_5.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZvX6kh9Pt4_",
        "cellView": "form"
      },
      "source": [
        "#@title BMMv2.5\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class BMMCUDAv2_5(CustomKernel): \n",
        "  def __init__(self, m=None, n=None, k=None, patch_m=4, patch_n=4):\n",
        "    super(BMMCUDAv2_5, self).__init__()\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = k\n",
        "    self.patch_m = patch_m\n",
        "    self.patch_n = patch_n\n",
        "    \n",
        "    with open(\"BMMCUDAv2_5.cu\",'r') as f: ###\n",
        "      self.kernel = f.read()\n",
        "      \n",
        "    self.kernel = (self.kernel\n",
        "      .replace(\"_M_\", str(m) if m else \"M\")\n",
        "      .replace(\"_N_\", str(n) if n else \"N\")\n",
        "      .replace(\"_K_\", str(k) if k else \"K\")\n",
        "      .replace(\"_PM_\", str(self.patch_m))\n",
        "      .replace(\"_PN_\", str(self.patch_n))\n",
        "    )\n",
        "    \n",
        "    self._fn_tt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nn\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "    # print(self._fn_nn.attributes)\n",
        "    self._fn_tn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "\n",
        "  def _call_nn(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A @ B\n",
        "      A: shape = [l, m, k]\n",
        "      B: shape = [l, k, n]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[1]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "    \n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_nn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k,\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_tt(self, A, B):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _call_tn(self, A, B):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _call_nt(self, A, B):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def __call__(self, A, B, mode=\"nn\"):\n",
        "    \"\"\"\n",
        "      Performs C = f(A) @ f(B)\n",
        "      A: torch.Tensor, shape : [l, m, k] or [l, k, m]\n",
        "      B: torch.Tensor, shape : [l, n, k] or [l, k, n]\n",
        "      returns C: torch.Tensor, shape : [l, m, n]\n",
        "      mode: str, default: \"nn\"\n",
        "      Notes:\n",
        "        f() and g() are determined by mode\n",
        "        \"nn\" --> A @ B\n",
        "        \"tt\" --> A.T @ B.T\n",
        "        \"nt\" --> A @ B.T\n",
        "        \"tn\" --> A.T @ B\n",
        "    \"\"\"\n",
        "    assert len(A.shape) == len(B.shape)\n",
        "    A = A.contiguous()\n",
        "    B = B.contiguous()\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      A2 = A[None]\n",
        "      B2 = B[None]\n",
        "    elif len(A.shape) == 3 and len(B.shape) == 3:\n",
        "      A2 = A\n",
        "      B2 = B\n",
        "    else:\n",
        "      raise ValueError(\"shape of A and B need to be 2d or 3d\")\n",
        "\n",
        "    if mode == \"nn\":\n",
        "      C = self._call_nn(A2, B2)\n",
        "    elif mode == \"tt\":\n",
        "      C = self._call_tt(A2, B2)\n",
        "    elif mode == \"tn\":\n",
        "      C = self._call_tn(A2, B2)\n",
        "    elif mode == \"nt\":\n",
        "      C = self._call_nt(A2, B2)\n",
        "\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      C = C[0]\n",
        "    return C"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asEodQyvJ_ex"
      },
      "source": [
        "## Note\n",
        "#### TopkBMM only works correctly when *M* and *N* are divisible by 128.  \n",
        "#### \"n_candidates\" is the k in topk.  \n",
        "#### *n_candidates* should be smaller than 128.  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CLWnHuSX9XrV",
        "cellView": "form"
      },
      "source": [
        "#@title test TopkBMM\n",
        "def test_topk_bmm(l, m, n, k, mode=\"nn\", n_iter=1, dim=1, n_candidates=128, verbose=0):\n",
        "  print(f\"l={l}  m={m}  n={n}  k={k}\")\n",
        "  if mode[0] == \"n\":\n",
        "    A = torch.randn(l, m, k, device=\"cuda:0\")\n",
        "  elif mode[0] == \"t\":\n",
        "    A = torch.randn(l, k, m, device=\"cuda:0\")\n",
        "  \n",
        "  if mode[1] == \"n\":\n",
        "    B = torch.randn(l, k, n, device=\"cuda:0\")\n",
        "  elif mode[1] == \"t\":\n",
        "    B = torch.randn(l, n, k, device=\"cuda:0\")\n",
        "  custom_topk_bmm = TopkBMMCUDA(patch_m=1, patch_n=16)\n",
        "  flop = l * m * n * k * 2 + l * m * n\n",
        "\n",
        "  if mode[0] == \"t\":\n",
        "    At = A.transpose(1, 2)\n",
        "  else: \n",
        "    At = A\n",
        "  if mode[1] == \"t\":\n",
        "    Bt = B.transpose(1, 2)\n",
        "  else:\n",
        "    Bt = B\n",
        "  #warmup\n",
        "\n",
        "  for i in range(n_iter):\n",
        "    C = torch.bmm(At, Bt)\n",
        "    C.topk(k=n_candidates, dim = dim)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "  tm = time()\n",
        "  for i in range(n_iter):\n",
        "    C = torch.bmm(At, Bt)\n",
        "    C_v, C_i = C.topk(k = n_candidates, dim = dim)\n",
        "    torch.cuda.synchronize()\n",
        "  time_cost_0 = (time() - tm) / n_iter\n",
        "  flops0 = (flop / time_cost_0) / 1000**4\n",
        "  if dim == 1:\n",
        "    C_v = C_v.transpose(1,2)\n",
        "    C_i = C_i.transpose(1,2)\n",
        "  if verbose > 0:\n",
        "    print(\"time spent for torch.bmm + min:\", time_cost_0)\n",
        "    print(\"tflops:\", flops0)\n",
        "  else:\n",
        "    del C_v, C_i \n",
        "  del C\n",
        "\n",
        "  # warmup\n",
        "  for i in range(n_iter):\n",
        "    custom_topk_bmm(A, B, mode=mode, dim=dim)\n",
        "    torch.cuda.synchronize()\n",
        "  tm = time()\n",
        "  for i in range(n_iter):\n",
        "    C1_v, C1_i = custom_topk_bmm(A, B, mode=mode, dim=dim, k=n_candidates)\n",
        "    torch.cuda.synchronize()\n",
        "  time_cost_1 = (time() - tm) / n_iter\n",
        "  flops1 = (flop / time_cost_1) / 1000**4\n",
        "  if verbose > 0:\n",
        "    print(\"time spent for custom_topk_bmm:\", time_cost_1)\n",
        "    print(\"tflops:\", flops1)\n",
        "  else:\n",
        "    del C1_v, C1_i\n",
        "\n",
        "  if verbose > 0:\n",
        "    val_dif = (C1_v - C_v).abs()\n",
        "    idx_dif = (C1_i != C_i)\n",
        "    print(\"Max Val Error\", val_dif.max())\n",
        "    print(\"Val Error:\", val_dif.sum())\n",
        "    print(\"Idx Error:\", idx_dif.sum())\n",
        "    print(\"ratio:\", time_cost_1 / time_cost_0)\n",
        "\n",
        "  return time_cost_0, time_cost_1\n",
        "  \n",
        "_ = test_topk_bmm(\n",
        "    1, 16384, 1024*2 + 512, 256,\n",
        "    mode=\"nn\", dim=1, n_iter=10, \n",
        "    n_candidates=128, verbose=1\n",
        ")\n",
        "# topk_dim1_v1 0.41 0.03\n",
        "# topk_dim1_v2 0.38 0.0305"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vXkMqJGQyPtj"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"imgs\"):\n",
        "  os.mkdir(\"imgs\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zeSM15FnoIDR",
        "cellView": "form"
      },
      "source": [
        "#@title Grid test TopkBMM\n",
        "ls = [1]\n",
        "ms = [256* 2**i for i in range(1, 3)]\n",
        "ns = [1024]\n",
        "\n",
        "ks = [1024]\n",
        "mode=\"nn\"\n",
        "\n",
        "custom_res = dict()\n",
        "cublass_res = dict()\n",
        "for l in ls:\n",
        "  for n in ns:\n",
        "    for m in ms:\n",
        "      for k in ks:     \n",
        "        res = test_topk_bmm(\n",
        "          l, m, n, k,\n",
        "          mode=mode, n_iter=15, dim=1,\n",
        "          n_candidates = 128,\n",
        "        )\n",
        "        cublass_res[m] = res[0]*1e3\n",
        "        custom_res[m] = res[1]*1e3\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10) )\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"N\", fontsize=17)\n",
        "plt.ylabel(\"milliseconds\", fontsize=17)\n",
        "title = f\"A[{l},N,{k}] B[{l},{k},{n}]\"\n",
        "plt.title(title)\n",
        "plt.rcParams[\"font.size\"] = \"17\"\n",
        "plt.grid()\n",
        "colors = [\"red\", \"blue\"]\n",
        "labels = [\"custom_topk_bmm\", \"torch.bmm -> torch.topk\"]\n",
        "for i, res in enumerate([custom_res, cublass_res]):\n",
        "  res_x = list(res.keys())\n",
        "  res_y = list(res.values())\n",
        "  # plt.plot(\n",
        "  # plt.loglog(\n",
        "  plt.semilogx(\n",
        "    res_x,\n",
        "    res_y,\n",
        "    color=colors[i],\n",
        "    label=labels[i],\n",
        "  )\n",
        "  # plt.plot(res_x, res_y, colors[i])\n",
        "plt.legend()\n",
        "plt.savefig(\"imgs/topk_bmm_\" + title + \"_semilogx\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}