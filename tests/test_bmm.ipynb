{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "test_bmm.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "GU4BcFUE5E0J"
      },
      "source": [
        "#!pip install --upgrade cupy-cuda112==8.5.0"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "75DR78cKpM3m"
      },
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import matplotlib.pyplot as plt\n",
        "import cupy as cp\n",
        "import math\n",
        "from time import time"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "form",
        "id": "12phuKJIFY9A"
      },
      "source": [
        "#@title CustomKernel\n",
        "import cupy as cp\n",
        "import torch\n",
        "\n",
        "@cp.util.memoize(for_each_device=True)\n",
        "def cunnex(func_name, func_body):\n",
        "  return cp.cuda.compile_with_cache(func_body).get_function(func_name)\n",
        "\n",
        "class Stream:\n",
        "  def __init__(self, ptr):\n",
        "    self.ptr = ptr\n",
        "  \n",
        "class CustomKernel:\n",
        "  def __init__(self):\n",
        "    self._use_torch_in_cupy_malloc()\n",
        "    self.stream = Stream(torch.cuda.current_stream().cuda_stream)\n",
        "\n",
        "  @staticmethod\n",
        "  def _torch_alloc(size):\n",
        "    device = cp.cuda.Device().id\n",
        "    tensor = torch.empty(size, dtype=torch.uint8, device=device)\n",
        "    return cp.cuda.MemoryPointer(\n",
        "        cp.cuda.UnownedMemory(tensor.data_ptr(), size, tensor), 0)\n",
        "\n",
        "  def _use_torch_in_cupy_malloc(self):\n",
        "    cp.cuda.set_allocator(self._torch_alloc)\n",
        "\n",
        "  def _compile_kernel_str(\n",
        "      self,\n",
        "      kernel,\n",
        "      name,\n",
        "      options=(),\n",
        "      backend=\"nvrtc\",\n",
        "      max_dynamic_smem=None\n",
        "    ):\n",
        "    fn = cp.RawKernel(\n",
        "      kernel,\n",
        "      name,\n",
        "      options=options,\n",
        "      backend=backend,\n",
        "    )\n",
        "    if max_dynamic_smem:\n",
        "      fn.max_dynamic_shared_size_bytes = max_dynamic_smem\n",
        "    return fn"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDuqX_JrO-50",
        "cellView": "form"
      },
      "source": [
        "#@title BMMv2.5 Kernel\n",
        "kernel = \"\"\"\n",
        "#define _VOLATILE_  \n",
        "\n",
        "#define likely(x)      __builtin_expect(!!(x), 1)\n",
        "#define unlikely(x)    __builtin_expect(!!(x), 0)\n",
        "#define load(x)        __ldcg(x)\n",
        "#define store(x, value) __stcs(x, value)\n",
        "\n",
        "typedef long long ll_t;\n",
        "typedef unsigned long long ull_t;\n",
        "\n",
        "typedef struct __builtin_align__(32) {\n",
        "  float s0, s1, s2, s3, s4, s5, s6, s7;\n",
        "} _float8;\n",
        "\n",
        "typedef union {\n",
        "  _float8 f8;\n",
        "  float val[8];\n",
        "} float8;\n",
        "\n",
        "__device__ void init_cCache(\n",
        "  float8 cCache[8]\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    #pragma unroll\n",
        "    for (int j=0; j<8; j++){\n",
        "      cCache[i].val[j] = 0.f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v4(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache1[8];\n",
        "  float aCache2[8];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    aCache1[mi] = aSM[0][8*vy + mi];\n",
        "  }\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    int is_odd = ki & 1;\n",
        "    if (is_odd == 0){\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache2[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache1[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    } else {\n",
        "      if (likely(ki < 7)){\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          aCache1[mi] = aSM[ki+1][8*vy + mi];\n",
        "        }\n",
        "      }\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "        #pragma unroll\n",
        "        for (int mi=0; mi<8; mi++){\n",
        "          float a = aCache2[mi];\n",
        "          cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul_v3(\n",
        "  _VOLATILE_ float aSM[8][128+4],\n",
        "  _VOLATILE_ float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int vy\n",
        ") {\n",
        "  float aCache[8];\n",
        "\n",
        "  #pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "    #pragma unroll\n",
        "    for (int mi=0; mi<8; mi++){\n",
        "      aCache[mi] = aSM[ki][8*vy + mi];\n",
        "    }\n",
        "    #pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      float b = bSM[ki][vx/4 + 8*vx + ni];\n",
        "      #pragma unroll\n",
        "      for (int mi=0; mi<8; mi++){\n",
        "        float a = aCache[mi];\n",
        "        cCache[mi].val[ni] = fmaf(a, b, cCache[mi].val[ni]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "// Unsafe\n",
        "__device__ void write_c(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    int iM = gStarty + vy*8 + i;\n",
        "    if (likely(iM < M)){\n",
        "      int iN_start = gStartx + vx*8;\n",
        "      reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      /*\n",
        "      if (likely(iN_start + 7 < N)){\n",
        "        reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (iN_start))[0] = cCache[i];\n",
        "      } else {\n",
        "        #pragma unroll\n",
        "        for (int j=0; j<8; j++){\n",
        "          int iN = iN_start + j;\n",
        "          if (iN < N){\n",
        "            C[(bid)*M*N + (iM)*N + (iN)] = cCache[i].val[j];\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "      */\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void write_c_v3(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "  __shared__ volatile float cSM[16][128];\n",
        "  #pragma unroll\n",
        "  for (int mi=0; mi<8; mi++){\n",
        "    int iM = gStarty + vy*8 + mi;\n",
        "    // Store 1 row from cCache to cSM\n",
        "    if (iM < M){\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        cSM[vy][vx*8 + ni] = cCache[mi].val[ni];\n",
        "      }\n",
        "      // Store to C\n",
        "      #pragma unroll\n",
        "      for (int ni=0; ni<8; ni++){\n",
        "        int iN = gStartx + 16*ni + vx;\n",
        "        if (iN < N){\n",
        "          float cVal = cSM[vy][16*ni + vx];\n",
        "          store(C+(bid)*M*N + (iM)*N + (iN), cVal);\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "  } \n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;     // thread idx\n",
        "  int bid = blockIdx.z;      // batch idx\n",
        "\n",
        "  // Neighboring blocks are grouped into PN x PM block groups in order to increase\n",
        "  // L1 cache hit rate\n",
        "  // There are ceil(M/PM) x ceil(N/PN) block groups in total.\n",
        "  // Blocks within block groups are indexed with blockIdx.x % PN and blockIdx.x / PN\n",
        "  int px = blockIdx.x % _PN_;\n",
        "  int py = blockIdx.x / _PN_;\n",
        "  int bDimX = (N + (128*_PN_) - 1) / (128*_PN_); \n",
        "  int bDimY = (M + (128*_PM_) - 1) / (128*_PM_); \n",
        "  int bIdxX = (blockIdx.y % bDimX) * _PN_ + px;\n",
        "  int bIdxY = (blockIdx.y / bDimX) * _PM_ + py;\n",
        "  int gStartx = bIdxX * 128;   // starting index of block on N axis\n",
        "  int gStarty = bIdxY * 128;   // starting index of block on M axis\n",
        "  if (gStartx > N || gStarty > M){\n",
        "    return;\n",
        "  }\n",
        "  // These are used to re-arrange threads into different shapes\n",
        "  // for example: (256) -> (16, 16) -> (8, 32) -> (32, 8)\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ _VOLATILE_ float aSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM1[8][128+4];\n",
        "  __shared__ _VOLATILE_ float aSM2[8][128+4];\n",
        "  __shared__ _VOLATILE_ float bSM2[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  // Load initial 16 x 128 tile of A and B to buffer1 and buffer2\n",
        "  #pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (likely(iM < _M_)){\n",
        "      if (likely(dx < _K_)){\n",
        "        aBuffer1[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (dx));\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(dx+8 < _K_)){\n",
        "        aBuffer2[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (dx+8));\n",
        "      } else {\n",
        "        aBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (likely(iN < N)){\n",
        "      if (likely(wy < _K_)){\n",
        "        bBuffer1[i] = load(B + (bid)*_N_*_K_ + (wy)*_N_ + (iN));\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "      if (likely(wy+8 < _K_)){\n",
        "        bBuffer2[i] = load(B + (bid)*_N_*_K_ + (wy+8)*_N_ + (iN));\n",
        "      } else {\n",
        "        bBuffer2[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "\n",
        "  // Number of main loop iterations is ceil(k/16)\n",
        "  int nIt = (_K_ + 16 - 1) / 16;\n",
        "  #pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    int gStartk = itr * 16;\n",
        "\n",
        "    // Index on K axis of A and B\n",
        "    int iKA = gStartk + 16 + dx;\n",
        "    int iKB = gStartk + 16 + wy;\n",
        "\n",
        "    #pragma unroll\n",
        "    for (int i=0; i<4; i++){\n",
        "      // Store buffered tiles into shared memory\n",
        "      aSM1[dx][dy+i*32] = aBuffer1[i];\n",
        "      bSM1[wy][wx+i*32+i] = bBuffer1[i];\n",
        "      aSM2[dx][dy+i*32] = aBuffer2[i];\n",
        "      bSM2[wy][wx+i*32+i] = bBuffer2[i];\n",
        "\n",
        "      // Start loading next 16*128 tile of A and B to buffer1 and buffer2.\n",
        "      // Don't load anything on the last iteration.\n",
        "      // Loading from global memory will not block thread_matmul\n",
        "      if (likely(itr < nIt - 1)){\n",
        "        int iM = gStarty + i*32 + dy;\n",
        "        int iN = gStartx + i*32 + wx;\n",
        "        \n",
        "        if (likely(iM < _M_)){\n",
        "          if (likely(iKA < _K_)){\n",
        "            aBuffer1[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (iKA));\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "          }\n",
        "          if (likely(iKA+8 < _K_)){\n",
        "            aBuffer2[i] = load(A + (bid)*_M_*_K_ + (iM)*_K_ + (iKA+8));\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "\n",
        "        if (likely(iN < _N_)){\n",
        "          if (likely(iKB < _K_)){\n",
        "            bBuffer1[i] = load(B + (bid)*_N_*_K_ + (iKB)*_N_ + (iN));\n",
        "          } else {\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "          if (likely(iKB+8 < _K_)){\n",
        "            bBuffer2[i] = load(B + (bid)*_N_*_K_ + (iKB+8)*_N_ + (iN));\n",
        "          } else {\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "      }\n",
        "    }\n",
        "    // synchroznie threads in order make sure tiles of A and B are fully\n",
        "    // loaded to shared memory.\n",
        "    __syncthreads();\n",
        "\n",
        "    // Each thread computes 8 x 8 matrix multiplication\n",
        "    // Accumulate intermediate results in cCache\n",
        "    // aSM1, bSM1, aSM2, bSM2 are consumed\n",
        "    thread_matmul_v3(aSM1, bSM1, cCache, vx, vy);\n",
        "    thread_matmul_v3(aSM2, bSM2, cCache, vx, vy);\n",
        "\n",
        "    // synchronize threads to signal that shared memory is consumed.\n",
        "    __syncthreads();\n",
        "  }\n",
        "  \n",
        "  // At the end of main loop, store cCache to C\n",
        "  //write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "  write_c_v3(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "\n",
        "  //C[bIdxY * N + bIdxX] = gStarty;\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"BMMCUDAv2_5.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZvX6kh9Pt4_",
        "cellView": "form"
      },
      "source": [
        "#@title BMMv2.5\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class BMMCUDAv2_5(CustomKernel): \n",
        "  def __init__(self, m=None, n=None, k=None, patch_m=4, patch_n=4):\n",
        "    super(BMMCUDAv2_5, self).__init__()\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = k\n",
        "    self.patch_m = patch_m\n",
        "    self.patch_n = patch_n\n",
        "    \n",
        "    with open(\"BMMCUDAv2_5.cu\",'r') as f: ###\n",
        "      self.kernel = f.read()\n",
        "      \n",
        "    self.kernel = (self.kernel\n",
        "      .replace(\"_M_\", str(m) if m else \"M\")\n",
        "      .replace(\"_N_\", str(n) if n else \"N\")\n",
        "      .replace(\"_K_\", str(k) if k else \"K\")\n",
        "      .replace(\"_PM_\", str(self.patch_m))\n",
        "      .replace(\"_PN_\", str(self.patch_n))\n",
        "    )\n",
        "    \n",
        "    self._fn_tt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nn\",\n",
        "      backend='nvcc',\n",
        "      options=(\n",
        "        '--maxrregcount=128',\n",
        "        '--use_fast_math',\n",
        "        #'-Xptxas',\n",
        "        #'-dlcm=cg',\n",
        "      )\n",
        "    )\n",
        "    # print(self._fn_nn.attributes)\n",
        "    self._fn_tn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "\n",
        "  def _call_nn(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A @ B\n",
        "      A: shape = [l, m, k]\n",
        "      B: shape = [l, k, n]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[1]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "    \n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    #blocks_per_grid = (math.ceil(n/128), math.ceil(m/128), l)\n",
        "    \n",
        "    n_ = math.ceil(n/(128*self.patch_n))\n",
        "    m_ = math.ceil(m/(128*self.patch_m))\n",
        "    blocks_per_grid = (self.patch_n*self.patch_m, n_ * m_, l)\n",
        "    # print(blocks_per_grid, m_, n_)\n",
        "\n",
        "    self._fn_nn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k,\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_tt(self, A, B):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _call_tn(self, A, B):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def _call_nt(self, A, B):\n",
        "    raise NotImplementedError\n",
        "\n",
        "  def __call__(self, A, B, mode=\"nn\"):\n",
        "    \"\"\"\n",
        "      Performs C = f(A) @ f(B)\n",
        "      A: torch.Tensor, shape : [l, m, k] or [l, k, m]\n",
        "      B: torch.Tensor, shape : [l, n, k] or [l, k, n]\n",
        "      returns C: torch.Tensor, shape : [l, m, n]\n",
        "      mode: str, default: \"nn\"\n",
        "      Notes:\n",
        "        f() and g() are determined by mode\n",
        "        \"nn\" --> A @ B\n",
        "        \"tt\" --> A.T @ B.T\n",
        "        \"nt\" --> A @ B.T\n",
        "        \"tn\" --> A.T @ B\n",
        "    \"\"\"\n",
        "    assert len(A.shape) == len(B.shape)\n",
        "    A = A.contiguous()\n",
        "    B = B.contiguous()\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      A2 = A[None]\n",
        "      B2 = B[None]\n",
        "    elif len(A.shape) == 3 and len(B.shape) == 3:\n",
        "      A2 = A\n",
        "      B2 = B\n",
        "    else:\n",
        "      raise ValueError(\"shape of A and B need to be 2d or 3d\")\n",
        "\n",
        "    if mode == \"nn\":\n",
        "      C = self._call_nn(A2, B2)\n",
        "    elif mode == \"tt\":\n",
        "      C = self._call_tt(A2, B2)\n",
        "    elif mode == \"tn\":\n",
        "      C = self._call_tn(A2, B2)\n",
        "    elif mode == \"nt\":\n",
        "      C = self._call_nt(A2, B2)\n",
        "\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      C = C[0]\n",
        "    return C"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0YaOjPguQNV",
        "cellView": "form"
      },
      "source": [
        "#@title BMM Kernel\n",
        "kernel = \"\"\"\n",
        "typedef long long ll_t;\n",
        "typedef unsigned long long ull_t;\n",
        "\n",
        "typedef struct __builtin_align__(32) {\n",
        "  float s0, s1, s2, s3, s4, s5, s6, s7;\n",
        "} _float8;\n",
        "\n",
        "typedef union {\n",
        "  _float8 f8;\n",
        "  float val[8];\n",
        "} float8;\n",
        "\n",
        "__device__ void init_cCache(\n",
        "  float8 cCache[8]\n",
        ") {\n",
        "#pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "#pragma unroll\n",
        "    for (int j=0; j<8; j++){\n",
        "      cCache[i].val[j] = 0.f;\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void SM2Cache(\n",
        "  float cache[8][4],\n",
        "  volatile float SM[8][128+4],\n",
        "  int vy, int p\n",
        ") {\n",
        "#pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "#pragma unroll\n",
        "    for (int mi=0; mi<4; mi++){\n",
        "      cache[ki][mi] = SM[ki][8*vy + 4*p + mi];\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void thread_matmul(\n",
        "  float aCache[8][4],\n",
        "  volatile float bSM[8][128+4],\n",
        "  float8 cCache[8],\n",
        "  int vx, int p\n",
        ") {\n",
        "#pragma unroll\n",
        "  for (int ki=0; ki<8; ki++){\n",
        "#pragma unroll\n",
        "    for (int ni=0; ni<8; ni++){\n",
        "      float b = bSM[ki][ vx/4 + 8*vx + ni];\n",
        "#pragma unroll\n",
        "      for (int mi=0; mi<4; mi++){\n",
        "        float a = aCache[ki][mi];\n",
        "        cCache[mi + 4*p].val[ni] = fmaf(a, b, cCache[mi + 4*p].val[ni]);\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "__device__ void write_c(\n",
        "  float8 cCache[8],\n",
        "  float* C,\n",
        "  int gStartx, int gStarty,\n",
        "  int vx, int vy, int bid,\n",
        "  int M, int N\n",
        ") {\n",
        "#pragma unroll\n",
        "  for (int i=0; i<8; i++){\n",
        "    int iM = gStarty + vy*8 + i;\n",
        "    if (iM < M){\n",
        "      reinterpret_cast<float8*>(C + (bid)*M*N + (iM)*N + (gStartx + vx*8))[0] = cCache[i];\n",
        "      /*\n",
        "#pragma unroll\n",
        "      for (int j=0; j<8; j++){\n",
        "        int iN = gStartx + vx*8 + j;\n",
        "        if (iN < N){\n",
        "          C[(bid)*M*N + (iM)*N + (iN)] = cCache[i].val[j];\n",
        "        }\n",
        "      }\n",
        "      */\n",
        "    }\n",
        "  }\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int bid = blockIdx.x;\n",
        "  int gStartx = blockIdx.y * 128;\n",
        "  int gStarty = blockIdx.z * 128;\n",
        "\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ volatile float aSM[8][128+4];\n",
        "  __shared__ volatile float bSM[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  int nIt = (_K_ + 8 - 1) / 8;\n",
        "  float init_value = 0.f;\n",
        "#pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "\n",
        "    int iM = gStarty + wx + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (wy < _K_){\n",
        "      if (iM < _M_)\n",
        "        aBuffer1[i] = A[(bid)*_M_*_K_ + (wy)*_M_ + (iM)];\n",
        "      if (iN < _N_)\n",
        "        bBuffer1[i] = B[(bid)*_N_*_K_ + (wy)*_N_ + (gStartx + wx + i*32)];\n",
        "    } else {\n",
        "      aBuffer1[i] = 0.f;\n",
        "      bBuffer1[i] = 0.f;\n",
        "    }\n",
        "  }\n",
        "#pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    \n",
        "    int gStartk = itr * 8;\n",
        "    int iK = gStartk + 8 + wy;\n",
        "    int is_odd = itr & 1;\n",
        "    if (is_odd == 0){\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + wx;\n",
        "          int iN = gStartx + i*32 + wx;\n",
        "          \n",
        "          if (iK < _K_){\n",
        "            if (iM < _M_)\n",
        "              aBuffer2[i] = A[(bid)*_M_*_K_ + (iK)*_M_ + (iM)];\n",
        "            if (iN < _N_)\n",
        "              bBuffer2[i] = B[(bid)*_N_*_K_ + (iK)*_N_ + (iN)];\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[wy][wx+i*32] = aBuffer1[i];\n",
        "        bSM[wy][wx+i*32+i] = bBuffer1[i];\n",
        "      }\n",
        "    } else {\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + wx;\n",
        "          int iN = gStartx + i*32 + wx;\n",
        "          if (iK < _K_){\n",
        "            if (iM < _M_)\n",
        "              aBuffer1[i] = A[(bid)*_M_*_K_ + (iK)*_M_ + (iM)];\n",
        "            if (iN < N)\n",
        "              bBuffer1[i] = B[(bid)*_N_*_K_ + (iK)*_N_ + (iN)];\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[wy][wx+i*32] = aBuffer2[i];\n",
        "        bSM[wy][wx+i*32+i] = bBuffer2[i];\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float aCache[8][4];\n",
        "\n",
        "#pragma unroll\n",
        "    for (int p=0; p<2; p++){\n",
        "      SM2Cache(aCache, aSM, vy, p);\n",
        "      // thread_matmul(aCache, bSM, cCache, vx, p);\n",
        "      thread_matmul(aCache, bSM, cCache, vx, p);\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int bid = blockIdx.x;\n",
        "  int gStartx = blockIdx.y * 128;\n",
        "  int gStarty = blockIdx.z * 128;\n",
        "\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ volatile float aSM[8][128+4];\n",
        "  __shared__ volatile float bSM[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  int nIt = (_K_ + 8 - 1) / 8;\n",
        "  float init_value = 0.f;\n",
        "#pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + dy + i*32;\n",
        "    if (dx < _K_){\n",
        "      if (iM < _M_)\n",
        "        aBuffer1[i] = A[(bid)*_M_*_K_ + (iM)*_K_ + (dx)];\n",
        "      if (iN < _N_)\n",
        "        bBuffer1[i] = B[(bid)*_N_*_K_ + (iN)*_K_ + (dx)];\n",
        "    } else {\n",
        "      aBuffer1[i] = 0.f;\n",
        "      bBuffer1[i] = 0.f;\n",
        "    }\n",
        "  }\n",
        "#pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    \n",
        "    int gStartk = itr * 8;\n",
        "    int iK = gStartk + 8 + dx;\n",
        "    int is_odd = itr & 1;\n",
        "    if (is_odd == 0){\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + dy;\n",
        "          int iN = gStartx + i*32 + dy;\n",
        "          \n",
        "          if (iK < _K_){\n",
        "            if (iM < _M_)\n",
        "              aBuffer2[i] = A[(bid)*_M_*_K_ + (iM)*_K_ + (iK)];\n",
        "            if (iN < _N_)\n",
        "              bBuffer2[i] = B[(bid)*_N_*_K_ + (iN)*_K_ + (iK)];\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[dx][dy+i*32] = aBuffer1[i];\n",
        "        bSM[dx][dy+i*32+i] = bBuffer1[i];\n",
        "      }\n",
        "    } else {\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + dy;\n",
        "          int iN = gStartx + i*32 + dy;\n",
        "          if (iK < _K_){\n",
        "            if (iM < _M_)\n",
        "              aBuffer1[i] = A[(bid)*_M_*_K_ + (iM)*_K_ + (iK)];\n",
        "            if (iN < N)\n",
        "              bBuffer1[i] = B[(bid)*_N_*_K_ + (iN)*_K_ + (iK)];\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[dx][dy+i*32] = aBuffer2[i];\n",
        "        bSM[dx][dy+i*32+i] = bBuffer2[i];\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float aCache[8][4];\n",
        "\n",
        "#pragma unroll\n",
        "    for (int p=0; p<2; p++){\n",
        "      SM2Cache(aCache, aSM, vy, p);\n",
        "      thread_matmul(aCache, bSM, cCache, vx, p);\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_nn(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int bid = blockIdx.x;\n",
        "  int gStartx = blockIdx.y * 128;\n",
        "  int gStarty = blockIdx.z * 128;\n",
        "\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ volatile float aSM[8][128+4];\n",
        "  __shared__ volatile float bSM[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  int nIt = (_K_ + 8 - 1) / 8;\n",
        "  float init_value = 0.f;\n",
        "#pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "\n",
        "    int iM = gStarty + dy + i*32;\n",
        "    int iN = gStartx + wx + i*32;\n",
        "    if (iM < _M_){\n",
        "      if (dx < _K_){\n",
        "        aBuffer1[i] = A[(bid)*_M_*_K_ + (iM)*_K_ + (dx)];\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (iN < N){\n",
        "      if (wy < _K_){\n",
        "        bBuffer1[i] = B[(bid)*_N_*_K_ + (wy)*_N_ + (iN)];\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "\n",
        "  }\n",
        "#pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    \n",
        "    int gStartk = itr * 8;\n",
        "    int iKA = gStartk + 8 + dx;\n",
        "    int iKB = gStartk + 8 + wy;\n",
        "    int is_odd = itr & 1;\n",
        "    if (is_odd == 0){\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + dy;\n",
        "          int iN = gStartx + i*32 + wx;\n",
        "          \n",
        "          if (iKA < _K_){\n",
        "            if (iM < _M_){\n",
        "              aBuffer2[i] = A[(bid)*_M_*_K_ + (iM)*_K_ + (iKA)];\n",
        "            }\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "          }\n",
        "\n",
        "          if (iKB < _K_){\n",
        "            if (iN < _N_){\n",
        "              bBuffer2[i] = B[(bid)*_N_*_K_ + (iKB)*_N_ + (iN)];\n",
        "            }\n",
        "          } else {\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[dx][dy+i*32] = aBuffer1[i];\n",
        "        bSM[wy][wx+i*32+i] = bBuffer1[i];\n",
        "      }\n",
        "    } else {\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + dy;\n",
        "          int iN = gStartx + i*32 + wx;\n",
        "\n",
        "          if (iKA < _K_){\n",
        "            if (iM < _M_){\n",
        "              aBuffer1[i] = A[(bid)*_M_*_K_ + (iM)*_K_ + (iKA)];\n",
        "            }\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "          }\n",
        "          \n",
        "\n",
        "          if (iKB < _K_){\n",
        "            if (iN < _N_){\n",
        "              bBuffer1[i] = B[(bid)*_N_*_K_ + (iKB)*_N_ + (iN)];\n",
        "            }\n",
        "          } else {\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[dx][dy+i*32] = aBuffer2[i];\n",
        "        bSM[wy][wx+i*32+i] = bBuffer2[i];\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float aCache[8][4];\n",
        "\n",
        "#pragma unroll\n",
        "    for (int p=0; p<2; p++){\n",
        "      SM2Cache(aCache, aSM, vy, p);\n",
        "      thread_matmul(aCache, bSM,cCache, vx, p);\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "\n",
        "  write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\n",
        "extern \"C\"\n",
        "__global__ void bmm_tt(\n",
        "  const float* __restrict__ A,\n",
        "  const float* __restrict__ B,\n",
        "  float* __restrict__ C,\n",
        "  int M, int N, int K\n",
        "){\n",
        "  int tid = threadIdx.x;\n",
        "  int bid = blockIdx.x;\n",
        "  int gStartx = blockIdx.y * 128;\n",
        "  int gStarty = blockIdx.z * 128;\n",
        "\n",
        "  int vx = tid % 16;\n",
        "  int vy = tid / 16;\n",
        "  int wx = tid % 32; // thread idx in warp\n",
        "  int wy = tid / 32; // warp id\n",
        "  int dx = tid % 8;\n",
        "  int dy = tid / 8;\n",
        "\n",
        "  __shared__ volatile float aSM[8][128+4];\n",
        "  __shared__ volatile float bSM[8][128+4];\n",
        "  float aBuffer1[4];\n",
        "  float bBuffer1[4];\n",
        "  float aBuffer2[4];\n",
        "  float bBuffer2[4];\n",
        "\n",
        "  float8 cCache[8];\n",
        "  init_cCache(cCache);\n",
        "\n",
        "  int nIt = (_K_ + 8 - 1) / 8;\n",
        "  float init_value = 0.f;\n",
        "#pragma unroll\n",
        "  for (int i=0; i<4; i++){\n",
        "\n",
        "    int iM = gStarty + wx + i*32;\n",
        "    int iN = gStartx + dy + i*32;\n",
        "    if (iM < _M_){\n",
        "      if (wy < _K_){\n",
        "        aBuffer1[i] = A[(bid)*_M_*_K_ + (wy)*_M_ + (iM)];\n",
        "      } else {\n",
        "        aBuffer1[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "    if (iN < _N_){\n",
        "      if (dx < _K_){\n",
        "        bBuffer1[i] = B[(bid)*_N_*_K_ + (iN)*_K_ + (dx)];\n",
        "      } else {\n",
        "        bBuffer1[i] = 0.f;\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "#pragma unroll\n",
        "  for (int itr=0; itr<nIt; itr++){\n",
        "    \n",
        "    int gStartk = itr * 8;\n",
        "    int iKA = gStartk + 8 + wy;\n",
        "    int iKB = gStartk + 8 + dx;\n",
        "    int is_odd = itr & 1;\n",
        "    if (is_odd == 0){\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + wx;\n",
        "          int iN = gStartx + i*32 + dy;\n",
        "          \n",
        "          if (iKA < _K_){\n",
        "            if (iM < _M_){\n",
        "              aBuffer2[i] = A[(bid)*_M_*_K_ + (iKA)*_M_ + (iM)];\n",
        "            }\n",
        "          } else {\n",
        "            aBuffer2[i] = 0.f;\n",
        "          }\n",
        "\n",
        "          if (iKB < _K_){\n",
        "            if (iN < _N_){\n",
        "              bBuffer2[i] = B[(bid)*_N_*_K_ + (iN)*_K_ + (iKB)];\n",
        "            }\n",
        "          } else {\n",
        "            bBuffer2[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[wy][wx+i*32] = aBuffer1[i];\n",
        "        bSM[dx][dy+i*32+i] = bBuffer1[i];\n",
        "      }\n",
        "    } else {\n",
        "#pragma unroll\n",
        "      for (int i=0; i<4; i++){\n",
        "        if (itr < nIt - 1){\n",
        "          int iM = gStarty + i*32 + wx;\n",
        "          int iN = gStartx + i*32 + dy;\n",
        "          if (iKA < _K_){\n",
        "            if (iM < _M_){\n",
        "              aBuffer1[i] = A[(bid)*_M_*_K_ + (iKA)*_M_ + (iM)];\n",
        "            }\n",
        "          } else {\n",
        "            aBuffer1[i] = 0.f;\n",
        "          }\n",
        "\n",
        "          if (iKB < _K_){\n",
        "            if (iN < _N_){\n",
        "              bBuffer1[i] = B[(bid)*_N_*_K_ + (iN)*_K_ + (iKB)];\n",
        "            }\n",
        "          } else {\n",
        "            bBuffer1[i] = 0.f;\n",
        "          }\n",
        "        }\n",
        "        aSM[wy][wx+i*32] = aBuffer2[i];\n",
        "        bSM[dx][dy+i*32+i] = bBuffer2[i];\n",
        "      }\n",
        "    }\n",
        "    __syncthreads();\n",
        "\n",
        "    float aCache[8][4];\n",
        "\n",
        "#pragma unroll\n",
        "    for (int p=0; p<2; p++){\n",
        "      SM2Cache(aCache, aSM, vy, p);\n",
        "      thread_matmul(aCache, bSM, cCache, vx, p);\n",
        "    }\n",
        "    __syncthreads();\n",
        "  }\n",
        "  write_c(cCache, C, gStartx, gStarty, vx, vy, bid, M, N);\n",
        "}\n",
        "\"\"\"\n",
        "with open(\"BMMCUDA.cu\", \"w\") as f:\n",
        "  f.write(kernel)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FkiN8PVjpTmM",
        "cellView": "form"
      },
      "source": [
        "#@title BMM\n",
        "import torch\n",
        "import cupy as cp\n",
        "import numpy as np\n",
        "import math\n",
        "\n",
        "class BMMCUDA(CustomKernel): \n",
        "  def __init__(self, m=None, n=None, k=None):\n",
        "    super(BMMCUDA, self).__init__()\n",
        "    self.m = m\n",
        "    self.n = n\n",
        "    self.k = k\n",
        "    # with open(get_absolute_path(\"BMMCUDA.cu\"),'r') as f: ###\n",
        "    with open(\"BMMCUDA.cu\",'r') as f: ###\n",
        "      self.kernel = f.read()\n",
        "      \n",
        "    self.kernel = (self.kernel\n",
        "      .replace(\"_M_\", str(m) if m else \"M\")\n",
        "      .replace(\"_N_\", str(n) if n else \"N\")\n",
        "      .replace(\"_K_\", str(k) if k else \"K\")\n",
        "    )\n",
        "    \n",
        "    self._fn_tt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_tn = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_tn\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "    self._fn_nt = cp.RawKernel(\n",
        "      code=self.kernel,\n",
        "      name=\"bmm_nt\",\n",
        "      backend='nvcc',\n",
        "      options=('--maxrregcount=128', '--use_fast_math')\n",
        "    )\n",
        "\n",
        "  def _call_nn(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A @ B\n",
        "      A: shape = [l, m, k]\n",
        "      B: shape = [l, k, n]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[1]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "    \n",
        "    l, m, k = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    blocks_per_grid = (l, math.ceil(n/128), math.ceil(m/128))\n",
        "\n",
        "    self._fn_nn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_tt(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A.t @ B.t\n",
        "      A: shape = [l, k, m]\n",
        "      B: shape = [l, n, k]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[1] == B.shape[2]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "    \n",
        "    l, k, m = A.shape\n",
        "    l, n, k = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    blocks_per_grid = (l, math.ceil(n/128), math.ceil(m/128))\n",
        "\n",
        "    self._fn_tt(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_tn(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A.t @ B\n",
        "      A: shape = [l, k, m]\n",
        "      B: shape = [l, k, n]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[1] == B.shape[1]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "\n",
        "    l, k, m = A.shape\n",
        "    l, k, n = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "    \n",
        "    threads_per_block = (256,)\n",
        "    blocks_per_grid = (l, math.ceil(n/128), math.ceil(m/128))\n",
        "\n",
        "    self._fn_tn(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k\n",
        "      ],\n",
        "      stream=self.stream,\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def _call_nt(self, A, B):\n",
        "    \"\"\"\n",
        "      Performs C = A @ B.t\n",
        "      A: shape = [l, m, k]\n",
        "      B: shape = [l, n, k]\n",
        "      returns C: shape = [l, m, n]\n",
        "    \"\"\"\n",
        "    assert A.shape[0] == B.shape[0]\n",
        "    assert A.shape[2] == B.shape[2]\n",
        "    assert A.device.type == \"cuda\"\n",
        "    assert B.device.type == \"cuda\"\n",
        "    assert A.dtype in (torch.float, torch.half)\n",
        "    assert B.dtype in (torch.float, torch.half)\n",
        "\n",
        "    l, m, k = A.shape\n",
        "    l, n, k = B.shape\n",
        "\n",
        "    if self.m is not None: assert m == self.m\n",
        "    if self.n is not None: assert n == self.n\n",
        "    if self.k is not None: assert k == self.k\n",
        "\n",
        "    C = torch.zeros([l, m, n], device=\"cuda:0\", dtype=A.dtype)\n",
        "\n",
        "    threads_per_block = (256,)\n",
        "    blocks_per_grid = (l, math.ceil(n/128), math.ceil(m/128))\n",
        "\n",
        "    self._fn_nt(\n",
        "      grid=blocks_per_grid,\n",
        "      block=threads_per_block,\n",
        "      args=[\n",
        "        A.data_ptr(),\n",
        "        B.data_ptr(),\n",
        "        C.data_ptr(),\n",
        "        m, n, k\n",
        "      ],\n",
        "      stream=self.stream\n",
        "    )\n",
        "    return C\n",
        "\n",
        "  def __call__(self, A, B, mode=\"nn\"):\n",
        "    \"\"\"\n",
        "      Performs C = f(A) @ f(B)\n",
        "      A: torch.Tensor, shape : [l, m, k] or [l, k, m]\n",
        "      B: torch.Tensor, shape : [l, n, k] or [l, k, n]\n",
        "      returns C: torch.Tensor, shape : [l, m, n]\n",
        "      mode: str, default: \"nn\"\n",
        "      Notes:\n",
        "        f() and g() are determined by mode\n",
        "        \"nn\" --> A @ B\n",
        "        \"tt\" --> A.T @ B.T\n",
        "        \"nt\" --> A @ B.T\n",
        "        \"tn\" --> A.T @ B\n",
        "    \"\"\"\n",
        "    assert len(A.shape) == len(B.shape)\n",
        "    A = A.contiguous()\n",
        "    B = B.contiguous()\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      A2 = A[None]\n",
        "      B2 = B[None]\n",
        "    elif len(A.shape) == 3 and len(B.shape) == 3:\n",
        "      A2 = A\n",
        "      B2 = B\n",
        "    else:\n",
        "      raise ValueError(\"shape of A and B need to be 2d or 3d\")\n",
        "\n",
        "    if mode == \"nn\":\n",
        "      C = self._call_nn(A2, B2)\n",
        "    elif mode == \"tt\":\n",
        "      C = self._call_tt(A2, B2)\n",
        "    elif mode == \"tn\":\n",
        "      C = self._call_tn(A2, B2)\n",
        "    elif mode == \"nt\":\n",
        "      C = self._call_nt(A2, B2)\n",
        "\n",
        "    if len(A.shape) == 2 and len(B.shape) == 2:\n",
        "      C = C[0]\n",
        "    return C"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QqGQHvq7QAzm",
        "cellView": "form"
      },
      "source": [
        "#@title test BMMv2_5\n",
        "def test_bmm_v2_5(l, m, n, k, mode=\"nn\", n_iter=1, verbose=0):\n",
        "  print(f\"l={l}  m={m}  n={n}  k={k}\")\n",
        "  if mode[0] == \"n\":\n",
        "    A = torch.randn(l, m, k, device=\"cuda:0\")\n",
        "  elif mode[0] == \"t\":\n",
        "    A = torch.randn(l, k, m, device=\"cuda:0\")\n",
        "  \n",
        "  if mode[1] == \"n\":\n",
        "    B = torch.randn(l, k, n, device=\"cuda:0\")\n",
        "  elif mode[1] == \"t\":\n",
        "    B = torch.randn(l, n, k, device=\"cuda:0\")\n",
        "  # custom_bmm = BMMCUDA()\n",
        "  custom_bmm_v2_5 = BMMCUDAv2_5(patch_m=4, patch_n=4)\n",
        "  flop = l * m * n * k * 2\n",
        "\n",
        "  if mode[0] == \"t\":\n",
        "    At = A.transpose(1, 2)\n",
        "  else: \n",
        "    At = A\n",
        "  if mode[1] == \"t\":\n",
        "    Bt = B.transpose(1, 2)\n",
        "  else:\n",
        "    Bt = B\n",
        "  #warmup\n",
        "  for i in range(n_iter):\n",
        "    torch.bmm(At, Bt)\n",
        "    torch.cuda.synchronize()\n",
        "\n",
        "  tm = time()\n",
        "  for i in range(n_iter):\n",
        "    C = torch.bmm(At, Bt)\n",
        "    # C2 = At @ Bt\n",
        "    torch.cuda.synchronize()\n",
        "  time_cost_2 = (time() - tm) / n_iter\n",
        "  flops2 = (flop / time_cost_2) / 1000**4\n",
        "  if verbose > 0:\n",
        "    print(\"time spent for torch.bmm:\", time_cost_2)\n",
        "    print(\"tflops:\", flops2)\n",
        "  else:\n",
        "    del C\n",
        "\n",
        "  # # warmup\n",
        "  # for i in range(n_iter):\n",
        "  #   custom_bmm(A, B, mode=mode)\n",
        "  # torch.cuda.synchronize()\n",
        "  # tm = time()\n",
        "  # for i in range(n_iter):\n",
        "  #   C4 = custom_bmm(A, B, mode=mode)\n",
        "  #   torch.cuda.synchronize()\n",
        "  # time_cost_4 = (time() - tm) / n_iter\n",
        "  # flops4 = (flop / time_cost_4) / 1000**4\n",
        "  # if verbose > 0:\n",
        "  #   print(\"time spent for custom_bmm:\", time_cost_4)\n",
        "  #   print(\"tflops:\", flops4)\n",
        "  # del C4\n",
        "\n",
        "  # warmup\n",
        "  for i in range(n_iter):\n",
        "    custom_bmm_v2_5(A, B, mode=mode)\n",
        "    torch.cuda.synchronize()\n",
        "  tm = time()\n",
        "  for i in range(n_iter):\n",
        "    C1 = custom_bmm_v2_5(A, B, mode=mode)\n",
        "    torch.cuda.synchronize()\n",
        "  time_cost_1 = (time() - tm) / n_iter\n",
        "  flops1 = (flop / time_cost_1) / 1000**4\n",
        "  if verbose > 0:\n",
        "    print(\"time spent for custom_bmm_v2_5:\", time_cost_1)\n",
        "    print(\"tflops:\", flops1)\n",
        "  else:\n",
        "    del C1\n",
        "\n",
        "  if verbose > 0:\n",
        "    dif = (C1 - C).abs()\n",
        "    print(\"Max Error\", dif.max())\n",
        "    print(\"Error:\", dif.sum())\n",
        "    print(\"ratio:\", time_cost_1 / time_cost_2)\n",
        "\n",
        "\n",
        "  if verbose > 1:\n",
        "    plt.imshow(( dif < 1e-4)[0].cpu())\n",
        "    plt.show()\n",
        "    plt.imshow(C1[0].cpu())\n",
        "    plt.show()\n",
        "\n",
        "  return time_cost_1, time_cost_2\n",
        "  \n",
        "_ = test_bmm_v2_5(1, 1024*16, 1024, 256,\n",
        "    mode=\"nn\", n_iter=100, verbose=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqFS7TjKLnSw"
      },
      "source": [
        "import os\n",
        "if not os.path.exists(\"imgs\"):\n",
        "  os.mkdir(\"imgs\")"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsZoADEyOH29",
        "cellView": "form"
      },
      "source": [
        "#@title Grid test BMM\n",
        "ls = [i*128 for i in range(1, 3)]\n",
        "ms = [512]\n",
        "ns = ms\n",
        "ks = [64]\n",
        "mode=\"nn\"\n",
        "\n",
        "custom_res = dict()\n",
        "cublass_res = dict()\n",
        "for l in ls:\n",
        "  for m in ms:\n",
        "    for k in ks:\n",
        "      res = test_bmm_v2_5(l, m, m, k, mode=mode, n_iter=50)\n",
        "      custom_res[l] = res[0] *1e3\n",
        "      cublass_res[l] = res[1] *1e3\n",
        "\n",
        "\n",
        "plt.figure(figsize=(15, 10) )\n",
        "plt.tight_layout()\n",
        "plt.xlabel(\"X\", fontsize=17)\n",
        "plt.ylabel(\"milliseconds\", fontsize=17)\n",
        "title = f\"A[X,{m},{k}] B[X,{k},{m}]\"\n",
        "plt.title(title)\n",
        "plt.rcParams[\"font.size\"] = \"17\"\n",
        "plt.grid()\n",
        "colors = [\"red\", \"blue\"]\n",
        "labels = [\"custom_bmm\", \"torch.bmm\"]\n",
        "for i, res in enumerate([custom_res, cublass_res]):\n",
        "  res_x = list(res.keys())\n",
        "  res_y = list(res.values())\n",
        "  plt.plot(\n",
        "    res_x,\n",
        "    res_y,\n",
        "    color=colors[i],\n",
        "    label=labels[i],\n",
        "  )\n",
        "plt.legend()\n",
        "plt.savefig(\"imgs/mbmm_\" + title)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}